{"pages":[{"title":"About","text":"","link":"/about/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Docker入门-第三篇（Services）","text":"以服务方式运行容器About services在分布式应用程序中，应用程序的不同部分称为“服务”。例如，想象一个视频共享站点，它可能包括一个用于将应用程序数据存储在数据库中的服务，一个用于在后台进行视频转码的服务。用户上传内容，前端服务等。服务实际上只是“生产环境中的容器”。服务只运行一个映像，但它编码了映像的运行方式 —— 它应该使用哪些端口，应该运行多少个容器副本，以便服务具有所需的容量等等。扩展服务会更改运行该软件的容器实例的数量，从而为流程中的服务分配更多计算资源。幸运的是，使用Docker平台定义，运行和扩展服务非常容易 —— 只需编写一个docker-compose.yml文件即可。 ## 创建第一个docker-compose.yml docker-compose.yml文件是一个YAML文件，用于定义Docker容器在生产环境中的行为方式。 docker-compose.yml 12345678910111213141516171819version: \"3\"services: web: # replace username/repo:tag with your name and image details image: username/repo:tag deploy: replicas: 5 resources: limits: cpus: \"0.1\" memory: 50M restart_policy: condition: on-failure ports: - \"4000:80\" networks: - webnetnetworks: webnet: 你可以将docker-compose.yml保存到任何地方。确保文件中的镜像名username/repo:tag替换为你想要的镜像，这个镜像是在镜像库中注册过的。 这个docker-compose.yml文件告诉Docker执行以下操作： 从注册表中提取镜像； 将该镜像的5个实例作为名为web的服务运行，限制每个实例使用最多10％的CPU（跨所有核心）和50MB的RAM; 如果一个失败，立即重启容器; 将主机上的端口4000映射到Web的端口80; 指示Web容器通过称为webnet的负载平衡网络共享端口80.（在内部，容器本身在短暂的端口发布到web的端口80.） 使用默认设置（负载平衡的覆盖网络）定义Webnet网络. 运行负载均衡后的App在我们使用docker stack deploy命令之前，我们首先运行： 1docker swarm init Note: 后面的文章中会详细介绍 swarm。如果您没有运行docker swarm init，则会收到“this node is not a swarm manager.”错误。 现在可以运行这个应用了。您需要为您的应用程序命名。在这里，它被设置为getstartedlab： 1docker stack deploy -c docker-compose.yml getstartedlab 我们的单个服务堆栈在一台主机上运行已部署了镜像的5个容器实例。通过一下方式，可以查看服务的ID: 123456789docker service ls############OUT PUT:# fangkui@fangkui-linux: ~/tmp (18:09:24) ζ docker service lsID NAME MODE REPLICAS IMAGE PORTSj6c3ps594ky1 getstartedlab_web replicated 5/5 friendlyhello:latest *:4000-&gt;80/tcp 在服务中运行的单个容器称为任务。任务被赋予以数字递增的唯一ID，最多为您在docker-compose.yml中定义的副本数。列出您的服务任务： 123456789101112docker service ps getstartedlab_web##########OUT PUT############# fangkui@fangkui-linux: ~/tmp (18:09:26) ζ docker service ps getstartedlab_webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSowxqt9zjb4hb getstartedlab_web.1 friendlyhello:latest fangkui-linux Running Running 2 minutes ago z5lm26k0ar4u getstartedlab_web.2 friendlyhello:latest fangkui-linux Running Running 2 minutes ago n4p8qtrhiegy getstartedlab_web.3 friendlyhello:latest fangkui-linux Running Running 2 minutes ago npzdltqqw9gb getstartedlab_web.4 friendlyhello:latest fangkui-linux Running Running 2 minutes ago n805z6c2smxn getstartedlab_web.5 friendlyhello:latest fangkui-linux Running Running 2 minutes ago 如果您只列出系统上的所有容器，则任务也会显示，但不会被服务过滤： 12345678# fangkui@fangkui-linux: ~/tmp (18:11:57) ζ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0c06e54d2901 friendlyhello:latest \"python app.py\" 3 minutes ago Up 3 minutes 80/tcp getstartedlab_web.4.npzdltqqw9gbgailbd7o09xcu64165df37b26 friendlyhello:latest \"python app.py\" 3 minutes ago Up 3 minutes 80/tcp getstartedlab_web.1.owxqt9zjb4hbgc1zw5r00wzr2fadfb984875b friendlyhello:latest \"python app.py\" 3 minutes ago Up 3 minutes 80/tcp getstartedlab_web.5.n805z6c2smxnmuuqf8runhnndc00f89869c3e friendlyhello:latest \"python app.py\" 3 minutes ago Up 3 minutes 80/tcp getstartedlab_web.2.z5lm26k0ar4ua9b36qbkql0uy28fe2522ca5c friendlyhello:latest \"python app.py\" 3 minutes ago Up 3 minutes 80/tcp getstartedlab_web.3.n4p8qtrhiegyrhzbju7omemy3 您可以连续多次运行curl -4 http：// localhost：4000，或者在浏览器中转到该URL并点击刷新几次。 无论哪种方式，容器ID都会发生变化，从而证明负载均衡;对于每个请求，以循环方式选择5个任务中的一个来响应。容器ID与上一个命令（docker container ls -q）的输出匹配。 扩展App您可以通过更改docker-compose.yml中的副本值来保存更改，并重新运行docker stack deploy命令来扩展应用程序： 1docker stack deploy -c docker-compose.yml getstartedlab Docker执行就地更新，无需首先拆除堆栈或杀死任何容器。现在，重新运行docker container ls -q以查看已重新配置的已部署实例。如果放大副本，则会启动更多任务，从而启动更多容器。 关闭App和Swarm关闭App: 1docker stack rm getstartedlab 关闭swarm: 1docker swarm leave --force Recap and cheat sheet (optional)12345678docker stack ls # List stacks or appsdocker stack deploy -c &lt;composefile&gt; &lt;appname&gt; # Run the specified Compose filedocker service ls # List running services associated with an appdocker service ps &lt;service&gt; # List tasks associated with an appdocker inspect &lt;task or container&gt; # Inspect task or containerdocker container ls -q # List container IDsdocker stack rm &lt;appname&gt; # Tear down an applicationdocker swarm leave --force # Take down a single node swarm from the manager Reference https://docs.docker.com/get-started/part3/#recap-and-cheat-sheet-optional","link":"/2018/08/28/Docker%E5%85%A5%E9%97%A8-%E7%AC%AC%E4%B8%89%E7%AF%87%EF%BC%88Services%EF%BC%89/"},{"title":"Docker入门-第二篇（Container）","text":"创建自己的Docker镜像变化在过去，如果您要开始编写Python应用程序，那么您的第一个任务是在您的计算机上安装Python运行时。但是，这会导致您的计算机上的环境需要非常适合您的应用程序按预期运行，并且还需要与您的生产环境相匹配。 使用Docker，您可以将可移植的Python运行时作为镜像，无需安装。您可以在构建的过程中，将应用及其依赖的基础python镜像打包，确保应用可以正确运行。 这些可移植镜像由称为Dockerfile的文件定义。 利用Dockerfile创建容器Dockerfile定义容器内环境中发生的事情。对网络接口和磁盘驱动器等资源的访问在此环境中进行虚拟化，该环境与系统的其他部分隔离，因此您需要将端口映射到外部世界，并具体说明要“复制”哪些文件到容器环境。但是，执行此操作后，您可以预期此Dockerfile中定义的应用程序的构建在其运行的任何位置都会完全相同。 Dockerfile创建一个空目录。将目录（cd）更改为新目录，创建名为Dockerfile的文件，将以下内容复制并粘贴到该文件中，然后保存： 1234567891011121314151617181920# Use an official Python runtime as a parent imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install --trusted-host pypi.python.org -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD [\"python\", \"app.py\"] 这个Dockerfile引用了我们尚未创建的几个文件，即app.py和requirements.txt。接下来创建这个两个文件。 应用实现在这里，我们继续创建两个文件，分别为requirements.txt，app.py。app.py是应用程序业务逻辑的实现，requirements.txt中的内容是应用所依赖的python包。将这两个文件放在与Dockerfile的同一个文件夹下。两个文件内容如下： requirements.txt 12FlaskRedis app.py 123456789101112131415161718192021222324from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host=\"redis\", db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route(\"/\")def hello(): try: visits = redis.incr(\"counter\") except RedisError: visits = \"&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;\" html = \"&lt;h3&gt;Hello {name}!&lt;/h3&gt;\" \\ \"&lt;b&gt;Hostname:&lt;/b&gt; {hostname}&lt;br/&gt;\" \\ \"&lt;b&gt;Visits:&lt;/b&gt; {visits}\" return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname(), visits=visits)if __name__ == \"__main__\": app.run(host='0.0.0.0', port=80) 现在我们看到pip install -r requirements.txt为Python安装了Flask和Redis库，应用程序打印环境变量NAME，以及对socket.gethostname（）的调用输出。最后，因为Redis没有运行（因为我们只安装了Python库，而不是Redis本身），这样在尝试使用Redis时会抛出错误。 构建应用现在我们已经准备好构建应用程序。确保您仍处于新目录的顶层。确保当你执行 ls 命令时，输出内容如下： 12$ lsDockerfile app.py requirements.txt 现在运行build命令。这将创建一个Docker镜像，这里使用 -t 参数为新构建的镜像命名。 1docker build -t friendlyhello . 现在可以在本地Docker镜像库中查看到我们构建的镜像： 1234567# fangkui@fangkui-linux: ~/tmpζ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEfriendlyhello latest 8c89b7cb2e32 3 days ago 132MBpython 2.7-slim 40792d8a2d6d 3 weeks ago 120MBhello-world latest 2cb0d9787c4d 6 weeks ago 1.85kB 运行创建的镜像运行应用程序，使用-p将计算机的端口4000映射到容器的已发布端口80： 1docker run -p 4000:80 friendlyhello 使用curl访问app，运行结果如下： 123# fangkui@fangkui-linux: ~ζ curl http://localhost:4000/&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 1e4ace406b41&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt; 可以通过添加-d选项，让容器运行于后台： 123docker run -d -p 4000:80 friendlyhello717fd03233dc6084724d13c18707b8d5c67694f8001b2f26724adce4216ac5df 您可以使用docker container ls查看缩写的容器ID： 12345# fangkui@fangkui-linux: ~/tmp (17:33:45) ζ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES717fd03233dc friendlyhello:latest \"python app.py\" 40 seconds ago Up 38 seconds 0.0.0.0:4000-&gt;80/tcp romantic_einstein Recap and cheat sheet (optional)12345678910111213141516docker build -t friendlyhello . # Create image using this directory's Dockerfiledocker run -p 4000:80 friendlyhello # Run \"friendlyname\" mapping port 4000 to 80docker run -d -p 4000:80 friendlyhello # Same thing, but in detached modedocker container ls # List all running containersdocker container ls -a # List all containers, even those not runningdocker container stop &lt;hash&gt; # Gracefully stop the specified containerdocker container kill &lt;hash&gt; # Force shutdown of the specified containerdocker container rm &lt;hash&gt; # Remove specified container from this machinedocker container rm $(docker container ls -a -q) # Remove all containersdocker image ls -a # List all images on this machinedocker image rm &lt;image id&gt; # Remove specified image from this machinedocker image rm $(docker image ls -a -q) # Remove all images from this machinedocker login # Log in this CLI session using your Docker credentialsdocker tag &lt;image&gt; username/repository:tag # Tag &lt;image&gt; for upload to registrydocker push username/repository:tag # Upload tagged image to registrydocker run username/repository:tag # Run image from a registry Reference https://docs.docker.com/get-started/part2/#recap-and-cheat-sheet-optional","link":"/2018/08/28/Docker%E5%85%A5%E9%97%A8-%E7%AC%AC%E4%BA%8C%E7%AF%87%EF%BC%88Container%EF%BC%89/"},{"title":"Docker入门-第四篇（Swarms）","text":"搭建Swarm集群Swarm简介Swarm指一组运行Docker并加入群集的计算机。针对一个Swarm，之前使用的Docker命令，现在由Swarm Manager在群集上执行。Swarm中的机器可以是物理的或虚拟的。加入群组后，它们被称为节点。 Swarm管理器可以使用多种策略来运行容器，例如“emptiest node” —— 它使用容器填充利用率最低的机器。或“global”，它确保每台机器只获得指定容器的一个实例。您可以指示Swarm Manager使用Compose文件中配置的策略，正如在上一篇文章所描述的那样。 Swarm Manager是Swarm中唯一可以执行命令的机器，或授权其他机器作为Worker加入Swarm。worker只是在那里提供资源，并且没有权力告诉任何其他机器它能做什么和不能做什么。 到目前为止，我们一直在本地计算机上以单主机模式使用Docker。但是Docker也可以切换到swarm模式。立即启用swarm mod使当前计算机成为swarm manager。这样，Docker就会运行您在管理的swarm上执行的命令，而不仅仅是在当前机器上。 创建Swarm一个Swarm由多个节点组成，这些节点可以是物理的计算机，也可以是虚拟的。最简单的方式是使用命令 docker swarm init初始化当前计算机为swarm manager。然后让其他计算机加入到创建的swarm。另一中方式是创建虚拟的swarm。这里我们创建一个具有两个虚拟节点的swarm。 创建虚拟节点 使用 docker-machine工具创建两个虚拟节点： 12docker-machine create --driver virtualbox myvm1docker-machine create --driver virtualbox myvm2 初始化swarm并添加节点 将myvm1配置为swarm manager，用于执行management commands，注册worker。设置myvm2为worker。 这里，利用命令 docker-machine ssh发送指令给虚拟机。 12345678910$ docker-machine ssh myvm1 \"docker swarm init --advertise-addr &lt;myvm1 ip&gt;\"Swarm initialized: current node &lt;node ID&gt; is now a manager.To add a worker to this swarm, run the following command:docker swarm join \\--token &lt;token&gt; \\&lt;myvm ip&gt;:&lt;port&gt;To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 注意替换ip地址。然后将myvm2添加到swarm中： 12345$ docker-machine ssh myvm2 \"docker swarm join \\--token &lt;token&gt; \\&lt;ip&gt;:2377\"This node joined a swarm as a worker. 部署App到Swarm 首先，需要切换环境变量，以便使用shell发送指令给swarm manager： 1eval $(docker-machine env myvm1) 跟前一篇文章类似，使用以下命令部署app： 1docker stack deploy -c docker-compose.yml getstartedlab 需要注意的是，我们前面创建的镜像并没有注册到远程镜像库，所以使用上面的命令部署app，会找不到镜像。可以通过在myvm1,myvm2中分别构建镜像来解决这个问题。 Cleanup and rebootStacks and swarms可以使用一下命令关闭stack： 1docker stack rm getstartedlab 恢复环境变量可以通过以下命令恢复环境变量： 1eval $(docker-machine env -u) Recap and cheat sheet (optional)123456789101112131415161718192021docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux)docker-machine create -d hyperv --hyperv-virtual-switch \"myswitch\" myvm1 # Win10docker-machine env myvm1 # View basic information about your nodedocker-machine ssh myvm1 \"docker node ls\" # List the nodes in your swarmdocker-machine ssh myvm1 \"docker node inspect &lt;node ID&gt;\" # Inspect a nodedocker-machine ssh myvm1 \"docker swarm join-token -q worker\" # View join tokendocker-machine ssh myvm1 # Open an SSH session with the VM; type \"exit\" to enddocker node ls # View nodes in swarm (while logged on to manager)docker-machine ssh myvm2 \"docker swarm leave\" # Make the worker leave the swarmdocker-machine ssh myvm1 \"docker swarm leave -f\" # Make master leave, kill swarmdocker-machine ls # list VMs, asterisk shows which VM this shell is talking todocker-machine start myvm1 # Start a VM that is currently not runningdocker-machine env myvm1 # show environment variables and command for myvm1eval $(docker-machine env myvm1) # Mac command to connect shell to myvm1&amp; \"C:\\Program Files\\Docker\\Docker\\Resources\\bin\\docker-machine.exe\" env myvm1 | Invoke-Expression # Windows command to connect shell to myvm1docker stack deploy -c &lt;file&gt; &lt;app&gt; # Deploy an app; command shell must be set to talk to manager (myvm1), uses local Compose filedocker-machine scp docker-compose.yml myvm1:~ # Copy file to node's home dir (only required if you use ssh to connect to manager and deploy the app)docker-machine ssh myvm1 \"docker stack deploy -c &lt;file&gt; &lt;app&gt;\" # Deploy an app using ssh (you must have first copied the Compose file to myvm1)eval $(docker-machine env -u) # Disconnect shell from VMs, use native dockerdocker-machine stop $(docker-machine ls -q) # Stop all running VMsdocker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk images Reference https://docs.docker.com/get-started/part4/#recap-and-cheat-sheet-optional","link":"/2018/08/28/Docker%E5%85%A5%E9%97%A8-%E7%AC%AC%E5%9B%9B%E7%AF%87%EF%BC%88Swarms%EF%BC%89/"},{"title":"Docker入门-第一篇","text":"Docker 基础知识Docker conceptsDocker是开发人员和系统管理员使用容器开发，部署和运行应用程序的平台。使用Linux容器部署应用程序称为容器化。 容器化越来越受欢迎的原因主要有一下几点： 灵活（Flexible）：即使是最复杂的应用也可以集装箱化。 轻量级（Lightweight）：容器利用并共享主机内核。 可互换（Interchangeable）：您可以即时部署更新和升级。 便携式（Portable）：您可以在本地构建，部署到云，并在任何地方运行。 可扩展（Scalable）：您可以增加并自动分发容器副本。 可堆叠（Stackable）：您可以垂直和即时堆叠服务。 Images and containers通过运行映像启动容器。映像是一个可执行包，包含运行应用程序所需的所有内容（代码，运行时，库，环境变量和配置文件）。 容器 是运行时 镜像的实例，包括镜像在执行时保存在内存中的信息（状态镜像，或者用户进程）。您可以使用命令docker ps查看正在运行的容器列表，就像在Linux中一样。 ![Container](/images/Container@2x.png) ![VM](/images/VM@2x.png) Containers and virtual machines容器在Linux上本机运行，并与其他容器共享主机的内核。它运行一个独立的进程，不占用任何其他可执行文件的内存，使其轻量级。相比之下，虚拟机（VM）运行一个完整的“客户”操作系统，通过虚拟机管理程序对主机资源进行虚拟访问。通常，VM提供的环境比大多数应用程序需要的资源更多。 Recap and cheat sheet12345678910111213141516171819## List Docker CLI commandsdockerdocker container --help## Display Docker version and infodocker --versiondocker versiondocker info## Execute Docker imagedocker run hello-world## List Docker imagesdocker image ls## List Docker containers (running, all, all in quiet mode)docker container lsdocker container ls --alldocker container ls -aq Reference https://docs.docker.com/get-started/#conclusion-of-part-one","link":"/2018/08/28/Docker%E5%85%A5%E9%97%A8/"},{"title":"ElasticSearch——Filter","text":"过滤查询及过滤使用场景分析在老版本的ElasticSearch中，过滤查询可以通过Filtered查询语句实现。但是在新版本中，Filtered查询语句已经被bool query取代。具体从哪个版本开始更改，由于没有找到相关资料不能确定。笔者在5.6.3版本中使用时已经发现不支持。 使用过滤查询的好处是，检索引擎只判断是否匹配查询条件，而不会对每个document进行打分排序。性能比较起来快于普通的query。 bool.filter在filter元素下指定的查询对评分没有影响 - 分数返回为0.分数仅受已指定查询的影响。例如，以下所有三个查询都返回状态字段包含术语“活动”的所有文档。 第一个查询为所有文档分配0分，因为没有指定评分查询： 123456789101112//GET _search{ \"query\": { \"bool\": { \"filter\": { \"term\": { \"status\": \"active\" } } } }} 在bool查询中添加match_all查询，为所有文档指定1.0分。 123456789101112131415//GET _search{ \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"term\": { \"status\": \"active\" } } } }} 下面的constant_score查询的行为方式与上面的第二个示例完全相同。 constant_score查询为过滤器匹配的所有文档指定1.0分。 123456789101112//GET _search{ &quot;query&quot;: { &quot;constant_score&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;status&quot;: &quot;active&quot; } } } }} 过滤查询和过滤聚合使用场景分析之前讨论过聚合查询的限定范围，聚合操作是在查询结果上进行实现的。如果没有查询语句，则默认为是在所有文档，类似于match_all。如下，单独使用过滤聚合时，返回的文档依旧是所有文档。但是聚合结果是根据过滤后的文档进行统计的。 123456789101112131415161718192021222324252627{ \"aggs\": { \"time\": { \"filter\": { \"range\": { \"detectionTime\": { \"lte\": \"2018-01-04 14:56:27\" } } } } }}// 查询结果\"hits\": { \"total\": 32621, \"max_score\": 1, \"hits\": [ ..... ] },\"aggregations\": { \"time\": { \"doc_count\": 5 }} 使用bool.filter查询，然后对过滤结果进行聚合，也可以直接达到过滤聚合的目的，并且返回的查询文档为过滤后的结果。 Reference https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html","link":"/2018/08/17/ElasticSearch%E2%80%94%E2%80%94Filter/"},{"title":"ElasticSearch——Lucene Scoring","text":"Lucene的实用评分函数对于多词查询， Lucene 使用 布尔模型（Boolean model） 、 TF/IDF 以及 向量空间模型（vector space model） ，然后将它们组合到单个高效的包里以收集匹配文档并进行评分计算。 一个多词查询 12345678GET /my_index/doc/_search{ \"query\": { \"match\": { \"text\": \"quick fox\" } }} 会在内部被重写为： 1234567891011GET /my_index/doc/_search{ \"query\": { \"bool\": { \"should\": [ {\"term\": { \"text\": \"quick\" }}, {\"term\": { \"text\": \"fox\" }} ] } }} bool 查询实现了布尔模型，在这个例子中，它会将包括词 quick 和 fox 或两者兼有的文档作为查询结果。 只要一个文档与查询匹配，Lucene 就会为查询计算评分，然后合并每个匹配词的评分结果。这里使用的评分计算公式叫做 实用评分函数（practical scoring function） 。看似很高大上，但是别被吓到——多数的组件都已经介绍过，下一步会讨论它引入的一些新元素。 123456789score(q,d) = queryNorm(q) · coord(q,d) · ∑ ( tf(t in d) · idf(t)² · t.getBoost() · norm(t,d) ) (t in q) score(q,d) 是文档 d 与查询 q 的相关度评分。 queryNorm(q) 是 查询归一化 因子 （新）。 coord(q,d) 是 协调 因子 （新）。 tf(t in d) 是词 t 在文档 d 中的 词频 。 idf(t) 是词 t 的 逆向文档频率 。 t.getBoost() 是查询中使用的 boost（新）。 norm(t,d) 是 字段长度归一值 ，与 索引时字段层 boost （如果存在）的和（新）。 上节已介绍过 score 、 tf 和 idf 。现在来介绍 queryNorm 、 coord 、 t.getBoost 和 norm 。 我们会在本章后面继续探讨 查询时的权重提升 的问题，但是首先需要了解查询归一化、协调和索引时字段层面的权重提升等概念。 查询归一因子查询归一因子 （ queryNorm ）试图将查询 归一化 ， 这样就能将两个不同的查询结果相比较。 尽管查询归一值的目的是为了使查询结果之间能够相互比较，但是它并不十分有效，因为相关度评分 _score 的目的是为了将当前查询的结果进行排序，比较不同查询结果的相关度评分没有太大意义。 这个因子是在查询过程的最前面计算的，具体的计算依赖于具体查询，一个典型的实现如下： 1queryNorm = 1 / √sumOfSquaredWeights sumOfSquaredWeights 是查询里每个词的 IDF 的平方和。 相同查询归一化因子会被应用到每个文档，不能被更改，总而言之，可以被忽略。 查询协调协调因子 （ coord ） 可以为那些查询词包含度高的文档提供奖励，文档里出现的查询词越多，它越有机会成为好的匹配结果。 设想查询 quick brown fox ，每个词的权重都是 1.5 。如果没有协调因子，最终评分会是文档里所有词权重的总和。例如： 文档里有 fox → 评分： 1.5 文档里有 quick fox → 评分： 3.0 文档里有 quick brown fox → 评分： 4.5 协调因子将评分与文档里匹配词的数量相乘，然后除以查询里所有词的数量，如果使用协调因子，评分会变成： 文档里有 fox → 评分： 1.5 * 1 / 3 = 0.5 文档里有 quick fox → 评分： 3.0 * 2 / 3 = 2.0 文档里有 quick brown fox → 评分： 4.5 * 3 / 3 = 4.5 协调因子能使包含所有三个词的文档比只包含两个词的文档评分要高出很多。 回想将查询 quick brown fox 重写成 bool 查询的形式： 123456789101112GET /_search{ \"query\": { \"bool\": { \"should\": [ { \"term\": { \"text\": \"quick\" }}, { \"term\": { \"text\": \"brown\" }}, { \"term\": { \"text\": \"fox\" }} ] } }} bool 查询默认会对所有 should 语句使用协调功能，不过也可以将其禁用。为什么要这样做？通常的回答是——无须这样。查询协调通常是件好事，当使用 bool 查询将多个高级查询如 match 查询包裹的时候，让协调功能开启是有意义的，匹配的语句越多，查询请求与返回文档间的重叠度就越高。 但在某些高级应用中，将协调功能关闭可能更好。设想正在查找同义词 jump 、 leap 和 hop 时，并不关心会出现多少个同义词，因为它们都表示相同的意思，实际上，只有其中一个同义词会出现，这是不使用协调因子的一个好例子： 12345678910111213GET /_search{ \"query\": { \"bool\": { \"disable_coord\": true, \"should\": [ { \"term\": { \"text\": \"jump\" }}, { \"term\": { \"text\": \"hop\" }}, { \"term\": { \"text\": \"leap\" }} ] } }} 当使用同义词的时候（参照： 同义词 ），Lucene 内部是这样的：重写的查询会禁用同义词的协调功能。 大多数禁用操作的应用场景是自动处理的，无须为此担心。 索引时字段层权重提升我们会讨论 查询时的权重提升，让字段 权重提升 就是让某个字段比其他字段更重要。 当然在索引时也能做到如此。实际上，权重的提升会被应用到字段的每个词，而不是字段本身。 将提升值存储在索引中无须更多空间，这个字段层索引时的提升值与字段长度归一值（参见 字段长度归一值 ）一起作为单个字节存于索引， norm(t,d) 是前面公式的返回值。 我们不建议在建立索引时对字段提升权重，有以下原因： 将提升值与字段长度归一值合在单个字节中存储会丢失字段长度归一值的精度，这样会导致 Elasticsearch 不知如何区分包含三个词的字段和包含五个词的字段。 要想改变索引时的提升值，就必须重新为所有文档建立索引，与此不同的是，查询时的提升值可以随着每次查询的不同而更改。 如果一个索引时权重提升的字段有多个值，提升值会按照每个值来自乘，这会导致该字段的权重急剧上升。 查询时赋予权重 是更为简单、清楚、灵活的选择。 了解了查询归一化、协同和索引时权重提升这些方式后，可以进一步了解相关度计算最有用的工具：查询时的权重提升。 Referencehttps://www.elastic.co/guide/cn/elasticsearch/guide/current/practical-scoring-function.html","link":"/2018/07/23/ElasticSearch%E2%80%94%E2%80%94Lucene-Scoring/"},{"title":"ElasticSearch——Aggregation","text":"ElasticSearch踩坑纪实——Aggregation在进行聚合查询的时候，默认会返回size大小的匹配的document。 聚合范围聚合可以与搜索请求同时执行，但是我们需要理解一个新概念： 范围 。 默认情况下，聚合与查询是对同一范围进行操作的，也就是说，聚合是基于我们查询匹配的文档集合进行计算的。 去重聚合使用Cardinality Aggregation可以统计根据某一字段，不重复值的总数。使用方法如下： 1234567891011// RestAPI 版本POST /sales/_search?size=0{ \"aggs\" : { \"type_count\" : { \"cardinality\" : { \"field\" : \"type\" } } }} 12345678// Java 版本AggregationBuilder aggregationBuilder = AggregationBuilders.cardinality(\"count\").field(\"deviceId\");SearchResponse response = requestBuilder.addAggregation(aggregationBuilder) .setSize(0) .execute() .actionGet();Cardinality agg = response.getAggregations().get(\"count\");int count = agg.getValue() 过滤聚合在聚合中插入子查询，可以对查询过后的结果进行聚合，具体使用方法如下： 123456789101112// RestAPI方式POST /sales/_search?size=0{ \"aggs\" : { \"t_shirts\" : { \"filter\" : { \"term\": { \"type\": \"t-shirt\" } }, \"aggs\" : { \"avg_price\" : { \"avg\" : { \"field\" : \"price\" } } } } }} 1234567891011121314// JAVA 方式SearchRequestBuilder requestBuilder = client.prepareSearch(\"program_running_normal\").setTypes(\"normal\");AggregationBuilder filterAggregationBuilder = AggregationBuilders.filter(\"count_by_time\", QueryBuilders.rangeQuery(\"endTime\").gte(\"2018-02-02 05:12:03\"));AggregationBuilder aggregationBuilder = AggregationBuilders.cardinality(\"count\").field(\"deviceId\");filterAggregationBuilder.subAggregation(aggregationBuilder);SearchResponse response = requestBuilder.addAggregation(filterAggregationBuilder) .setSize(0) .execute() .actionGet();Filter filter = response.getAggregations().get(\"count_by_time\");Cardinality agg = filter.getAggregations().get(\"count\");long count = agg.getValue(); 在上面的例子当中，在过滤聚合中添加子查询作为过滤条件，然后添加子聚合进额外的聚合操作。返回结果如下： 123456789{ ... \"aggregations\" : { \"t_shirts\" : { \"doc_count\" : 3, \"avg_price\" : { \"value\" : 128.33333333333334 } } }} 所以在Java中，先从作为父聚合的过滤聚合获取到子聚合，最后得到统计数据。 Date Histogram Aggregation利用时间直方图聚合，可以很方便获取每个时间间隔内的document数。具体使用方法如下： 1234567891011{ \"aggs\" : { \"sales_over_time\" : { \"date_histogram\" : { \"field\" : \"date\", \"interval\" : \"1M\", \"format\" : \"yyyy-MM-dd\" } } }} 返回结果如下： 123456789101112131415161718192021222324{ ... \"aggregations\": { \"sales_over_time\": { \"buckets\": [ { \"key_as_string\": \"2015-01-01\", \"key\": 1420070400000, \"doc_count\": 3 }, { \"key_as_string\": \"2015-02-01\", \"key\": 1422748800000, \"doc_count\": 2 }, { \"key_as_string\": \"2015-03-01\", \"key\": 1425168000000, \"doc_count\": 2 } ] } }} 从结果看出，通过时间直方图聚合，可以将文档分桶进行聚合，每个桶存储一个时间段内的所有文档。 Date Histogram Aggregation留的坑使用ElasticSearch自带的直方图聚合可以很方便地获取根据时间统计的直方图数据。但是，直接使用会出现一些问题，下面举个栗子。 具体场景是这样，在项目中需要获得基于时间的直方图数据，时间范围为48h，分为12个时间段，所以时间间隔为4h。下面给出查询语句： 123456789101112131415161718192021222324252627282930313233343536373839404142434445{ \"size\": 0, \"aggs\": { \"last_hours\": { \"filter\": { \"range\": { \"detectionTime\": { \"from\": \"2018-08-29 15:00:27\", \"to\": \"2018-08-31 15:00:27\", \"format\": \"YYYY-MM-dd HH:mm:ss\" } } }, \"aggs\": { \"isrisk\": { \"filter\": { \"term\": { \"isEnvironmentRisk.keyword\": \"Y\" } }, \"aggs\": { \"riskytrend\": { \"date_histogram\": { \"field\": \"detectionTime\", \"interval\": \"240m\", \"min_doc_count\": 0, \"extended_bounds\": { \"min\": \"2018-08-29 15:00:27\", \"max\": \"2018-08-31 15:00:27\" } }, \"aggs\": { \"uniq_attr\": { \"cardinality\": { \"field\": \"deviceId\" } } } } } } } } }} 查询的返回结果如下： 12345678910111213{\"key_as_string\": \"2018-08-29 12:00:00\", \"key\": 1535544000000, \"doc_count\": 5, \"uniq_attr\":{\"value\": 2…},{\"key_as_string\": \"2018-08-29 16:00:00\", \"key\": 1535558400000, \"doc_count\": 10, \"uniq_attr\":{\"value\": 2…},{\"key_as_string\": \"2018-08-29 20:00:00\", \"key\": 1535572800000, \"doc_count\": 0, \"uniq_attr\":{\"value\": 0…},{\"key_as_string\": \"2018-08-30 00:00:00\", \"key\": 1535587200000, \"doc_count\": 0, \"uniq_attr\":{\"value\": 0…},{\"key_as_string\": \"2018-08-30 04:00:00\", \"key\": 1535601600000, \"doc_count\": 0, \"uniq_attr\":{\"value\": 0…},{\"key_as_string\": \"2018-08-30 08:00:00\", \"key\": 1535616000000, \"doc_count\": 2, \"uniq_attr\":{\"value\": 1…},{\"key_as_string\": \"2018-08-30 12:00:00\", \"key\": 1535630400000, \"doc_count\": 1, \"uniq_attr\":{\"value\": 1…},{\"key_as_string\": \"2018-08-30 16:00:00\", \"key\": 1535644800000, \"doc_count\": 3, \"uniq_attr\":{\"value\": 1…},{\"key_as_string\": \"2018-08-30 20:00:00\", \"key\": 1535659200000, \"doc_count\": 0, \"uniq_attr\":{\"value\": 0…},{\"key_as_string\": \"2018-08-31 00:00:00\", \"key\": 1535673600000, \"doc_count\": 0, \"uniq_attr\":{\"value\": 0…},{\"key_as_string\": \"2018-08-31 04:00:00\", \"key\": 1535688000000, \"doc_count\": 0, \"uniq_attr\":{\"value\": 0…},{\"key_as_string\": \"2018-08-31 08:00:00\", \"key\": 1535702400000, \"doc_count\": 10, \"uniq_attr\":{\"value\": 2…},{\"key_as_string\": \"2018-08-31 12:00:00\", \"key\": 1535716800000, \"doc_count\": 4, \"uniq_attr\":{\"value\": 1…} 仔细查看上面的数据，根据官方文档的分桶原则 数据在 [0,50) 落在桶0，在 [50,100) 落在桶50，以此类推，理论上我们需要的分桶是 [2018-08-29 11:00:00, 2018-08-29 15:00:00), [2018-08-29 15:00:00, 2018-08-29 19:00:00) ….。但是查询所得结果中的第一个桶从 2018-08-29 12:00:00进行统计，不符合我们的实际需求。导致这个的主要原因就在于ES的 bucket_key的计算算法，如下： 1Math.floor((value - offset) / interval) * interval + offset offset参数的具体作用请参考官方文档，当使用这个算法对数值进行计算时，可能不会产生较大偏差（这里笔者没有测试）。我们写一个小测试，使用上面的算法计算对应时间的 bucket_key 看看究竟发生了什么，代码如下： 123456public void time() throws ParseException { Date now = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(\"2018-08-31 14:21:36\"); logger.info(\"Now: {}\", new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(now)); Date start = new Date((long) (Math.floor((now.getTime()/1000.0/60.0 - 0)/240.0)*240.0+0)*1000*60); logger.info(\"Start: {}\", new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(start));} 执行上面的代码，得到如下日志输出： 122018-08-31 16:12:19.432 [main] INFO c.p.a.s.w.s.i.OverviewServiceImplTest - Now: 2018-08-31 14:21:362018-08-31 16:12:19.434 [main] INFO c.p.a.s.w.s.i.OverviewServiceImplTest - Start: 2018-08-31 12:00:00 我们看到时间 2018-08-31 14:21:36 计算出来的 bucket_key 为 12:00:00，相差如此巨大。为了解决这个问题，我们可以在Date Histogram中添加 offset参数。offset可以通过动态计算得到： 123456789private String offsetHelper(String from, String to, Integer group) throws ParseException { Date start = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(from); Date firstBucket = new Date( (long) (Math.floor( (start.getTime() / 1000.0 / 60.0 - 0) / intervalHelper(from, to, group)) * intervalHelper(from, to, group) + 0) * 1000 * 60); return String.valueOf(DateUtil.timeDiffOnHour(start, firstBucket) * 60 - intervalHelper(from, to, group)) + \"m\";} 上面的代码主要思想是，计算 bucket_key和实际时间在时间小时上的差，然后用这个时间差减去分桶间隔得到的值可以作为offset。 子聚合通过在聚合中添加子聚合，可以对父聚合过滤后的document进行聚合。如，使用date_histogram聚合对时间进行分桶之后，需要对每个桶中进行去重聚合（cardinality）。具体做法如下： 12345678910111213141516171819202122232425262728293031{ \"size\": 0, \"aggs\": { \"last_hours\": { \"filter\": { \"range\": { \"endTime\": { \"from\": \"2018-02-02 05:12:03\", \"to\": \"2018-02-03 05:12:03\", \"format\": \"YYYY-MM-dd HH:mm:ss\" } } }, \"aggs\": { \"runningtrend\": { \"date_histogram\": { \"field\": \"endTime\", \"interval\": \"10m\" }, \"aggs\": { \"uniq_attr\": { \"cardinality\": { \"field\": \"deviceId\" } } } } } } }} 注意上面的写法，命名为 uniq_attr的子聚合是和父聚合 runningtrend的聚合类型 date_histogram同级的。笔者开始将子聚合和父聚合的命名放在同级，得到的结果便是两个聚合并不是嵌套关系，而是并列的关系。正确语句得到的结果如下： 12345678910111213141516171819202122232425262728{ \"aggregations\": { \"last_hours\": { \"doc_count\": 18622, \"runningtrend\": { \"buckets\": [ { \"key_as_string\": \"2018-02-02 05:10:00\", \"key\": 1517548200000, \"doc_count\": 407, \"uniq_attr\": { \"value\": 1 } }, { \"key_as_string\": \"2018-02-02 05:20:00\", \"key\": 1517548800000, \"doc_count\": 511, \"uniq_attr\": { \"value\": 1 } } ....... ] } } }} Reference https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-aggregations-bucket-filter-aggregation.html","link":"/2018/08/16/ElasticSearch%E2%80%94%E2%80%94Aggregation/"},{"title":"ElasticSearch——Java API","text":"ElasticSearch JAVA API: DSL Query精确查找当进行精确值查找时， 我们会使用过滤器（filters）。过滤器很重要，因为它们执行速度非常快，不会计算相关度（直接跳过了整个评分阶段）而且很容易被缓存。我们会在本章后面的 过滤器缓存 中讨论过滤器的性能优势，不过现在只要记住：请尽可能多的使用过滤式查询。 通常当查找一个精确值的时候，我们不希望对查询进行评分计算。只希望对文档进行包括或排除的计算，所以我们会使用 constant_score 查询以非评分模式来执行 term 查询并以一作为统一评分。 最终组合的结果是一个 constant_score 查询，它包含一个 term 查询： 123456789101112GET /my_store/products/_search{ \"query\" : { \"constant_score\" : { \"filter\" : { \"term\" : { \"price\" : 20 } } } }} Java API对应实现如下： 1234// version6.3SearchResponse response = requestBuilder.setQuery(QueryBuilders.constantScoreQuery( QueryBuilder.termsQuery(\"price\", 20) )).setSize(100).execute().actionGet(); 精确查找文本对文本进行精确查找时，会出现问题，如果在索引数据时，没有指定not_analyzed。那么在索引时，文本会被分词： 12345GET /my_store/_analyze{ \"field\": \"productID\", \"text\": \"XHDK-A-1293-#fJ3\"} 123456789101112131415161718192021222324252627{ \"tokens\" : [ { \"token\" : \"xhdk\", \"start_offset\" : 0, \"end_offset\" : 4, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"a\", \"start_offset\" : 5, \"end_offset\" : 6, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 }, { \"token\" : \"1293\", \"start_offset\" : 7, \"end_offset\" : 11, \"type\" : \"&lt;NUM&gt;\", \"position\" : 3 }, { \"token\" : \"fj3\", \"start_offset\" : 13, \"end_offset\" : 16, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 4 } ]} 为了避免这种情况，可以在索引的时候指定 not_analyzed 查找多个精确值不需要使用多个 term 查询，我们只要用单个 terms 查询（注意末尾的 s ）， terms 查询好比是 term 查询的复数形式（以英语名词的单复数做比）。 它几乎与 term 的使用方式一模一样，与指定单个价格不同，我们只要将 term 字段的值改为数组即可： 12345{ \"terms\" : { \"price\" : [20, 30] }} 对应Java API实现如下： 12345values.add(20);values.add(30);SearchResponse response = requestBuilder.setQuery(QueryBuilders.constantScoreQuery( QueryBuilders.termsQuery(\"price\", values))).setSize(100).execute().actionGet(); SearchTypees在查询时，可以指定搜索类型为: QUERY_THEN_FETCH,QUERY_AND_FEATCH,DFS_QUERY_THEN_FEATCH和DFS_QUERY_AND_FEATCH（SACN,COUNT都已不建议使用）。那么这4种搜索类型有什么区别？ elasticsearch java api中还有个default 1public static final SearchType DEFAULT = QUERY_THEN_FETCH; 分布式搜索过程一个搜索请求必须询问我们关注的索引（index or indices）的所有分片的某个副本来确定它们是否含有任何匹配的文档。 但是找到所有的匹配文档仅仅完成事情的一半。 在 search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为 query then fetch 。 查询阶段在初始 查询阶段 时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的 _优先队列_。 查询阶段包含以下三个步骤: 客户端发送一个 search 请求到 Node 3 ， Node 3 会创建一个大小为 from + size 的空优先队列。 Node 3 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。 第一步是广播请求到索引中每一个节点的分片拷贝。就像 document GET requests 所描述的， 查询请求可以被某个主分片或某个副本分片处理， 这就是为什么更多的副本（当结合更多的硬件）能够增加搜索吞吐率。 协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。 每个分片在本地执行查询请求并且创建一个长度为 from + size 的优先队列—也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如 _score 。 协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。 一个索引可以由一个或几个主分片组成， 所以一个针对单个索引的搜索请求需要能够把来自多个分片的结果组合起来。 针对 multiple 或者 all 索引的搜索工作方式也是完全一致的–仅仅是包含了更多的分片而已。 取回阶段查询阶段标识哪些文档满足 搜索请求，但是我们仍然需要取回这些文档。 分布式阶段由以下步骤构成： 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。 每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。 一旦所有的文档都被取回了，协调节点返回结果给客户端。 协调节点首先决定哪些文档 确实 需要被取回。例如，如果我们的查询指定了 { “from”: 90, “size”: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片。 协调节点给持有相关文档的每个分片创建一个 multi-get request ，并发送请求给同样处理查询阶段的分片副本。 分片加载文档体– _source 字段–如果有需要，用元数据和 search snippet highlighting 丰富结果文档。 一旦协调节点接收到所有的结果文档，它就组装这些结果为单个响应返回给客户端。 问题第一、数量问题。比如，用户需要搜索”双黄连”，要求返回最符合条件的前10条。但在5个分片中，可能都存储着双黄连相关的数据。所以ES会向这5个分片都发出查询请求，并且要求每个分片都返回符合条件的10条记录。当ES得到返回的结果后，进行整体排序，然后取最符合条件的前10条返给用户。这种情况，ES5个shard最多会收到10*5=50条记录，这样返回给用户的结果数量会多于用户请求的数量。 第二、排名问题。上面搜索，每个分片计算分值都是基于自己的分片数据进行计算的。计算分值使用的词频率和其他信息都是基于自己的分片进行的，而ES进行整体排名是基于每个分片计算后的分值进行排序的，这就可能会导致排名不准确的问题。如果我们想更精确的控制排序，应该先将计算排序和排名相关的信息（词频率等）从5个分片收集上来，进行统一计算，然后使用整体的词频率去每个分片进行查询。 这两个问题，估计ES也没有什么较好的解决方法，最终把选择的权利交给用户，方法就是在搜索的时候指定query type。 query and fetch向索引的所有分片（shard）都发出查询请求，各分片返回的时候把元素文档（document）和计算后的排名信息一起返回。这种搜索方式是最快的。因为相比下面的几种搜索方式，这种查询方法只需要去shard查询一次。但是各个shard返回的结果的数量之和可能是用户要求的size的n倍。 query then fetch（默认的搜索方式）如果你搜索时，没有指定搜索方式，就是使用的这种搜索方式。这种搜索方式，大概分两个步骤，第一步，先向所有的shard发出请求，各分片只返回排序和排名相关的信息（注意，不包括文档document)，然后按照各分片返回的分数进行重新排序和排名，取前size个文档。然后进行第二步，去相关的shard取document。这种方式返回的document可能是用户要求的size的n倍 DFS query and fetch这种方式比第一种方式多了一个初始化散发(initial scatter)步骤，有这一步，据说可以更精确控制搜索打分和排名。这种方式返回的document与用户要求的size是相等的。 DFS query then fetch比第2种方式多了一个初始化散发(initial scatter)步骤。这种方式返回的document与用户要求的size是相等的。 DSF是什么缩写？初始化散发是一个什么样的过程？从es的官方网站我们可以指定，初始化散发其实就是在进行真正的查询之前，先把各个分片的词频率和文档频率收集一下，然后进行词搜索的时候，各分片依据全局的词频率和文档频率进行搜索和排名。显然如果使用DFS_QUERY_THEN_FETCH这种查询方式，效率是最低的，因为一个搜索，可能要请求3次分片。但，使用DFS方法，搜索精度应该是最高的。至于DFS是什么缩写，没有找到相关资料，这个D可能是Distributed，F可能是frequency的缩写，至于S可能是Scatter的缩写，整个单词可能是分布式词频率和文档频率散发的缩写。总结一下，从性能考虑QUERY_AND_FETCH是最快的，DFS_QUERY_THEN_FETCH是最慢的。从搜索的准确度来说，DFS要比非DFS的准确度更高。 Referencehttps://www.elastic.co/guide/cn/elasticsearch/guide/current/_query_phase.html https://blog.csdn.net/wangmaohong0717/article/details/53433146","link":"/2018/07/23/ElasticSearch%E2%80%94%E2%80%94Java-API/"},{"title":"ElasticSearch——Scroll","text":"ElasticSearch中的分页查询在ElasticSreach中可以使用setFrome(size)来实现分页查询，具体查询方法如下： 1234567{ \"size\":100, \"frome\":1000, \"query\" : { \"term\" : { \"user\" : \"kimchy\" } }} 123456SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index) .setTypes(type) .setQuery(boolQueryBuilder) .addSort(sortQueryBuild) .setFrom(skipSize) .setSize(pageSize); 但是这种分页方式有个限制条件，size+from不能超过10000。除了这种方式，ElasticSearch提供了另外一个方法进行分页查询，scroll。 Scrollsearch 请求返回一个单一的结果“页”，而 scroll API 可以被用来检索大量的结果（甚至所有的结果），就像在传统数据库中使用的游标 cursor。 scroll并不是为了实时的用户响应，而是为了处理大量的数据，例如，为了使用不同的配置来重新索引一个 index 到另一个 index 中去。 为了使用 scroll，初始搜索请求应该在查询中指定 scroll 参数，这可以告诉 Elasticsearch 需要保持搜索的上下文环境多久。 123456789POST /twitter/_search?scroll=1m{ \"size\": 100, \"query\": { \"match\" : { \"title\" : \"elasticsearch\" } }} 使用上面的请求返回的结果中包含一个 scroll_id，这个 ID 可以被传递给 scroll API 来检索下一个批次的结果。注意，这里在获取下一个批次的结果时候，不需要指定index和type。 12345POST /_search/scroll{ \"scroll\" : \"1m\", \"scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\"} JavaAPI进行Scroll如下： 1234567891011SearchResponse scrollResponse = client.prepareSearch(INDEX) .setSearchType(SearchType.SCAN).setSize(10000).setScroll(TimeValue.timeValueMinutes(1)) .execute().actionGet(); count = scrollResponse.getHits().getTotalHits();//第一次不返回数据for(int i=0,sum=0; sum&lt;count; i++){ scrollResponse = client.prepareSearchScroll(scrollResponse.getScrollId()) .setScroll(TimeValue.timeValueMinutes(8)) .execute().actionGet(); sum += scrollResponse.getHits().hits().length; System.out.println(\"总量\"+count+\" 已经查到\"+sum);} 清除 scroll API搜索上下文当 scroll 超时就会自动移除。但是保持 scroll 存活需要代价，所以 当scroll不再被使用的时候可以通过 clear-scroll 显式地清除： 1234DELETE /_search/scroll{ \"scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\"} 同样，上面的操作不需要指定index和type。 可以通过传递All参数，删除当前存在的所有Scroll。 1DELETE /_search/scroll/_all Reference https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-request-scroll.html","link":"/2018/08/16/ElasticSearch%E2%80%94%E2%80%94Scroll/"},{"title":"ElasticSearch——scoring-theory","text":"ElasticSearch scoring theoryLucene（或 Elasticsearch）使用 布尔模型（Boolean model） 查找匹配文档， 并用一个名为 实用评分函数（practical scoring function） 的公式来计算相关度。这个公式借鉴了 词频/逆向文档频率（term frequency/inverse document frequency） 和 向量空间模型（vector space model），同时也加入了一些现代的新特性，如协调因子（coordination factor），字段长度归一化（field length normalization），以及词或查询语句权重提升。 布尔模型布尔模型（Boolean Model） 只是在查询中使用 AND 、 OR 和 NOT （与、或和非）这样的条件来查找匹配的文档，以下查询： 1full AND text AND search AND (elasticsearch OR lucene) 会将所有包括词 full 、 text 和 search ，以及 elasticsearch 或 lucene 的文档作为结果集。 这个过程简单且快速，它将所有可能不匹配的文档排除在外。 词频/逆向文档频率(TF/IDF)当匹配到一组文档后，需要根据相关度排序这些文档，不是所有的文档都包含所有词，有些词比其他的词更重要。一个文档的相关度评分部分取决于每个查询词在文档中的 权重 。 词的权重由三个因素决定. 词频词在文档中出现的频度是多少？ 频度越高，权重 越高 。 5 次提到同一词的字段比只提到 1 次的更相关。词频的计算方式如下： 1tf(t in d) = √frequency 词 t 在文档 d 的词频（ tf ）是该词在文档中出现次数的平方根。 如果不在意词在某个字段中出现的频次，而只在意是否出现过，则可以在字段映射中禁用词频统计： 12345678910111213PUT /my_index{ \"mappings\": { \"doc\": { \"properties\": { \"text\": { \"type\": \"string\", \"index_options\": \"docs\" } } } }} 将参数 index_options 设置为 docs 可以禁用词频统计及词频位置，这个映射的字段不会计算词的出现次数，对于短语或近似查询也不可用。要求精确查询的 not_analyzed 字符串字段会默认使用该设置。 逆向文档频率词在集合所有文档里出现的频率是多少？频次越高，权重 越低 。 常用词如 and 或 the 对相关度贡献很少，因为它们在多数文档中都会出现，一些不常见词如 elastic 或 hippopotamus 可以帮助我们快速缩小范围找到感兴趣的文档。逆向文档频率的计算公式如下： 1idf(t) = 1 + log ( numDocs / (docFreq + 1)) 词 t 的逆向文档频率（ idf ）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。 字段长度归一值字段的长度是多少？ 字段越短，字段的权重 越高 。如果词出现在类似标题 title 这样的字段，要比它出现在内容 body 这样的字段中的相关度更高。字段长度的归一值公式如下： 1norm(d) = 1 / √numTerms 字段长度归一值（ norm ）是字段中词数平方根的倒数。 字段长度的归一值对全文搜索非常重要， 许多其他字段不需要有归一值。无论文档是否包括这个字段，索引中每个文档的每个 string 字段都大约占用 1 个 byte 的空间。对于 not_analyzed 字符串字段的归一值默认是禁用的，而对于 analyzed 字段也可以通过修改字段映射禁用归一值： 12345678910111213PUT /my_index{ \"mappings\": { \"doc\": { \"properties\": { \"text\": { \"type\": \"string\", \"norms\": { \"enabled\": false } } } } }} 这个字段不会将字段长度归一值考虑在内，长字段和短字段会以相同长度计算评分。 对于有些应用场景如日志，归一值不是很有用，要关心的只是字段是否包含特殊的错误码或者特定的浏览器唯一标识符。字段的长度对结果没有影响，禁用归一值可以节省大量内存空间。 结合使用以下三个因素——词频（term frequency）、逆向文档频率（inverse document frequency）和字段长度归一值（field-length norm）——是在索引时计算并存储的。 最后将它们结合在一起计算单个词在特定文档中的 权重 。 前面公式中提到的 文档 实际上是指文档里的某个字段，每个字段都有它自己的倒排索引，因此字段的 TF/IDF 值就是文档的 TF/IDF 值。 当用 explain 查看一个简单的 term 查询时（参见 explain ），可以发现与计算相关度评分的因子就是前面章节介绍的这些： 1234567891011PUT /my_index/doc/1{ \"text\" : \"quick brown fox\" }GET /my_index/doc/_search?explain{ \"query\": { \"term\": { \"text\": \"fox\" } }} 以上请求（简化）的 explanation 解释如下： 12345678weight(text:fox in 0) [PerFieldSimilarity]: 0.15342641 //词 fox 在文档的内部 Lucene doc ID 为 0 ，字段是 text 里的最终评分。result of: fieldWeight in 0 0.15342641 product of: tf(freq=1.0), with freq of 1: 1.0 //词 fox 在该文档 text 字段中只出现了一次。 idf(docFreq=1, maxDocs=1): 0.30685282 //fox 在所有文档 text 字段索引的逆向文档频率。 fieldNorm(doc=0): 0.5 // 该字段的字段长度归一值。 当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector space model）。 向量空间模型向量空间模型（vector space model） 提供一种比较多词查询的方式，单个评分代表文档与查询的匹配程度，为了做到这点，这个模型将文档和查询都以 向量（vectors） 的形式表示： 向量实际上就是包含多个数的一维数组，例如： 1[1,2,5,22,3,8] 在向量空间模型里， 向量空间模型里的每个数字都代表一个词的 权重 ，与 词频/逆向文档频率（term frequency/inverse document frequency） 计算方式类似。 尽管 TF/IDF 是向量空间模型计算词权重的默认方式，但不是唯一方式。Elasticsearch 还有其他模型如 Okapi-BM25 。TF/IDF 是默认的因为它是个经检验过的简单又高效的算法，可以提供高质量的搜索结果。 设想如果查询 “happy hippopotamus” ，常见词 happy 的权重较低，不常见词 hippopotamus 权重较高，假设 happy 的权重是 2 ， hippopotamus 的权重是 5 ，可以将这个二维向量—— [2,5] ——在坐标系下作条直线，线的起点是 (0,0) 终点是 (2,5) ，如下图 “表示 “happy hippopotamus” 的二维查询向量” 。 现在，设想我们有三个文档： I am happy in summer 。 After Christmas I’m a hippopotamus 。 The happy hippopotamus helped Harry 。 可以为每个文档都创建包括每个查询词—— happy 和 hippopotamus ——权重的向量，然后将这些向量置入同一个坐标系中，如图 图 28 ““happy hippopotamus” 查询及文档向量” ： 文档 1： (happy,____) —— [2,0] 文档 2： ( ___ ,hippopotamus) —— [0,5] 文档 3： (happy,hippopotamus) —— [2,5] 向量之间是可以比较的，只要测量查询向量和文档向量之间的角度就可以得到每个文档的相关度，文档 1 与查询之间的角度最大，所以相关度低；文档 2 与查询间的角度较小，所以更相关；文档 3 与查询的角度正好吻合，完全匹配。 在实际中，只有二维向量（两个词的查询）可以在平面上表示，幸运的是， 线性代数 ——作为数学中处理向量的一个分支——为我们提供了计算两个多维向量间角度工具，这意味着可以使用如上同样的方式来解释多个词的查询。关于比较两个向量的更多信息可以参考 余弦近似度（cosine similarity）。 Referencehttps://www.elastic.co/guide/cn/elasticsearch/guide/current/scoring-theory.html#boolean-model","link":"/2018/07/23/ElasticSearch%E2%80%94%E2%80%94scoring-theory/"},{"title":"Hexo Next 配置gitment插件","text":"Gitment插件配置问题由于笔者在定义文章名字时文章名太长，所以会导致评论初始化不成功。根据网上大佬的解答，是因为gitment产生的评论是通过github repo issue进行记录的。但是issue的label字符长度不能超过50。而next主题默认label获取方式为：window.location.pathname，很容易超过长度限制。 解决方式还是根据网上大佬们提供的方案，可以对 window.location.pathname 的值进行MD5编码，这样得到的结果可以保证长度不超过限制。具体方法如下，这里我使用的Next主题版本为 6.3.0 找到Next引入gitment的代码所在文件，在 6.3.0版本中该文件路径为：/next/layout/_third-party/comments/gitment.swig 在文件中引入第三方MD5编码工具包： 123{% if theme.gitment.enable and theme.gitment.client_id %} &lt;script src=&quot;https://cdn.bootcss.com/blueimp-md5/1.1.0/js/md5.js&quot;&gt;&lt;/script&gt; 对 window.location.pathname进行MD5编码，具体方法为找到上面文件中设置id字段值的地方，然后替换为 md5(window.location.pathname)修改后的文件内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859{% if theme.gitment.enable and theme.gitment.client_id %} &lt;script src=&quot;https://cdn.bootcss.com/blueimp-md5/1.1.0/js/md5.js&quot;&gt;&lt;/script&gt; &lt;!-- LOCAL: You can save these files to your site and update links --&gt; {% if theme.gitment.mint %} {% set CommentsClass = &quot;Gitmint&quot; %} &lt;link rel=&quot;stylesheet&quot; href=&quot;https://aimingoo.github.io/gitmint/style/default.css&quot;&gt; &lt;script src=&quot;https://aimingoo.github.io/gitmint/dist/gitmint.browser.js&quot;&gt;&lt;/script&gt; {% else %} {% set CommentsClass = &quot;Gitment&quot; %} &lt;link rel=&quot;stylesheet&quot; href=&quot;https://imsun.github.io/gitment/style/default.css&quot;&gt; &lt;script src=&quot;https://imsun.github.io/gitment/dist/gitment.browser.js&quot;&gt;&lt;/script&gt; {% endif %}&lt;!-- END LOCAL --&gt; {% if theme.gitment.cleanly %} &lt;style&gt; a.gitment-editor-footer-tip { display: none; } .gitment-container.gitment-footer-container { display: none; } &lt;/style&gt; {% endif %} {% if page.comments %} &lt;script type=&quot;text/javascript&quot;&gt; function renderGitment(){ var gitment = new {{CommentsClass}}({ id: md5(window.location.pathname), owner: '{{ theme.gitment.github_user }}', repo: '{{ theme.gitment.github_repo }}', {% if theme.gitment.mint %} lang: &quot;{{ theme.gitment.language }}&quot; || navigator.language || navigator.systemLanguage || navigator.userLanguage, {% endif %} oauth: { {% if theme.gitment.mint and theme.gitment.redirect_protocol %} redirect_protocol: '{{ theme.gitment.redirect_protocol }}', {% endif %} {% if theme.gitment.mint and theme.gitment.proxy_gateway %} proxy_gateway: '{{ theme.gitment.proxy_gateway }}', {% else %} client_secret: '{{ theme.gitment.client_secret }}', {% endif %} client_id: '{{ theme.gitment.client_id }}' }}); gitment.render('gitment-container'); } {% if not theme.gitment.lazy %} renderGitment(); {% else %} function showGitment(){ document.getElementById(&quot;gitment-display-button&quot;).style.display = &quot;none&quot;; document.getElementById(&quot;gitment-container&quot;).style.display = &quot;block&quot;; renderGitment(); } {% endif %} &lt;/script&gt; {% endif %}{% endif %} Reference https://www.v2ex.com/t/381246","link":"/2018/08/20/Hexo-Next-%E9%85%8D%E7%BD%AEgitment%E6%8F%92%E4%BB%B6/"},{"title":"GTSRB德国交通标志","text":"German Traffic Sign Recognition Benchmark数据获取地址：http://benchmark.ini.rub.de/ 数据可以免费使用。但是，如果您使用该数据，我们诚挚地要求您引用以下出版物： J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1453–1460. 2011. 数据结构从网站上可以获取到训练集和测试集，训练集文件名如下： 文件名 描述 GTSRB_Final_Training_Images.zip 交通标志图像，以及注释文件 GTSRB_Final_Training_HueHist.zip 对于训练集中的每个图像，该文件包含一个256色块色调值直方图（HSV色彩空间）。 GTSRB_Final_Training_HOG.zip 该文件包含三组不同配置的HOG特征（定向梯度直方图）。这些集合分别包含长度为1568,1568和2916的特征向量。 GTSRB_Final_Training_Haar.zip 该文件包含一组类哈尔特征。对于每幅图像，总共可以计算出12种不同特征的5种不同类型的哈尔特征。整体特征向量包含11,584个特征。 测试集文件名如下： 文件名 描述 GTSRB_Final_Test_Images.zip 交通标志图像 GTSRB_Final_Test_HueHist.zip 对于测试集中的每个图像，该文件包含一个256色块色调值直方图（HSV色彩空间）。 GTSRB_Final_Test_HOG.zip HOG特征 GTSRB_Final_Test_Haar.zip Haar-like features GTSRB_Final_Test_GT.zip 包含类别的注释文件 图像 每个图像包含一个交通标志 图像在实际交通标志周围（至少5个像素）包含10％的边界，以允许基于边缘的方法 图像以PPM格式存储（Portable Pixmap, P6） 图像大小在15x15到250x250像素之间变化 图像不一定是正方形的 实际的交通标志不一定以图像为中心。对于在完整摄像机图像中接近图像边界的图像，这是正确的 交通标志的边界框是annotatinos的一部分（见下文） 注释（annotatino）注释在CSV文件中提供。字段由“;”分隔（分号）。注释包含以下信息： Filename: Filename of corresponding image Width: Width of the image Height: Height of the image ROI.x1: X-coordinate of top-left corner of traffic - sign bounding box ROI.y1: Y-coordinate of top-left corner of traffic - sign bounding box ROI.x2: X-coordinate of bottom-right corner of traffic sign bounding box ROI.y2: Y-coordinate of bottom-right corner of traffic sign bounding box 训练数据注释将另外包含: ClassId: Assigned class label 数据读取代码示例Python示例代码提供了一个函数来遍历训练集以读取图像和相应的类ID。 代码依赖于matplotlib。 123456789101112131415161718192021222324252627282930313233343536373839# The German Traffic Sign Recognition Benchmark## sample code for reading the traffic sign images and the# corresponding labels## example:## trainImages, trainLabels = readTrafficSigns('GTSRB/Training')# print len(trainLabels), len(trainImages)# plt.imshow(trainImages[42])# plt.show()## have fun, Christianimport matplotlib.pyplot as pltimport csv# function for reading the images# arguments: path to the traffic sign data, for example './GTSRB/Training'# returns: list of images, list of corresponding labelsdef readTrafficSigns(rootpath): '''Reads traffic sign data for German Traffic Sign Recognition Benchmark. Arguments: path to the traffic sign data, for example './GTSRB/Training' Returns: list of images, list of corresponding labels''' images = [] # images labels = [] # corresponding labels # loop over all 42 classes for c in range(0,43): prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file gtReader.next() # skip header # loop over all images in current annotations file for row in gtReader: images.append(plt.imread(prefix + row[0])) # the 1th column is the filename labels.append(row[7]) # the 8th column is the label gtFile.close() return images, labels","link":"/2018/03/18/German-Traffic-Sign-Recognition-Benchmark/"},{"title":"JavaScript笔记——Object.prototype.hasOwnProperty()","text":"Object.prototype.hasOwnProperty()Object原型原型方法prototype.hasOwnProperty()hasOwnProperty() 方法会返回一个布尔值，指示对象 自身 属性中是否具有指定的属性 语法 obj.hasOwnProperty(prop) 参数 prop_，要检测的属性 _字符串 名称或者 Symbol。 返回值 用来判断某个对象是否含有指定的属性的 Boolean 。 描述 所有继承了 Object 的对象都会继承到 hasOwnProperty 方法。这个方法可以用来检测一个对象是否含有特定的自身属性；和 in 运算符不同，该方法会忽略掉那些从原型链上继承到的属性。 示例1234567891011121314151617o = new Object();o.prop = 'exists';function changeO() { o.newprop = o.prop; delete o.prop;}o.hasOwnProperty('prop'); // 返回 truechangeO();o.hasOwnProperty('prop'); // 返回 falseo = new Object();o.prop = 'exists';o.hasOwnProperty('prop'); // 返回 trueo.hasOwnProperty('toString'); // 返回 falseo.hasOwnProperty('hasOwnProperty'); // 返回 false 使用hasOwnProperty作为属性名JavaScript 并没有保护 hasOwnProperty 属性名，因此某个对象是有可能重写该属性，可以使用外部的 hasOwnProperty 获得正确的结果： 1234567891011121314var foo = { hasOwnProperty: function() { return false; }, bar: 'Here be dragons'};foo.hasOwnProperty('bar'); // 始终返回 false// 如果担心这种情况，可以直接使用原型链上真正的 hasOwnProperty 方法({}).hasOwnProperty.call(foo, 'bar'); // true// 也可以使用 Object 原型上的 hasOwnProperty 属性Object.prototype.hasOwnProperty.call(foo, 'bar'); // true","link":"/2018/05/30/JavaScript%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Object.prototype.hasOwnProperty()/"},{"title":"JavaScript笔记————this","text":"JavaScript中的this与其他语言相比，函数的 this 关键字在 JavaScript 中的表现略有不同，此外，在严格模式和非严格模式之间也会有一些差别。 在绝大多数情况下，函数的调用方式决定了this的值。this不能在执行期间被赋值，并且在每次函数被调用时this的值也可能会不同。ES5引入了bind方法来设置函数的this值，而不用考虑函数如何被调用的，ES2015 引入了支持this词法解析的箭头函数（它在闭合的执行上下文内设置this的值）。 全局上下文无论是否在严格模式下，在全局执行上下文中（在任何函数体外部）this 都指代全局对象。 123456789// 在浏览器中, window 对象同时也是全局对象：console.log(this === window); // truea = 37;console.log(window.a); // 37this.b = \"MDN\";console.log(window.b) // \"MDN\"console.log(b) // \"MDN\" 函数上下文在函数内部，this的值取决于函数被调用的方式。 简单调用因为下面的代码不在严格模式下，且 this 的值不是由该调用设置的，所以 this 的值默认指向全局对象。 12345678function f1(){ return this;}//在浏览器中：f1() === window; //在浏览器中，全局对象是window//在Node中：f1() === global; 然而，在严格模式下，this将保持他进入执行上下文时的值，所以下面的this将会默认为undefined。 123456function f2(){ \"use strict\"; // 这里是严格模式 return this;}f2() === undefined; // true 所以，在 严格模式下，如果 this 没有被执行上下文（execution context）定义，那它将保持为 undefined。 在第二个例子中，this的确应该是undefined，因为f2是被直接调用的，而不是作为对象的属性或方法调用的（如 window.f2()）。有一些浏览器最初在支持严格模式时没有正确实现这个功能，于是它们错误地返回了window对象。 如果要想把 this 的值从一个上下文传到另一个，就要用 call 或者apply 方法。 12345678910111213// 将一个对象作为call和apply的第一个参数，this会被绑定到这个对象。var obj = {a: 'Custom'};// 这个属性是在global对象定义的。var a = 'Global';function whatsThis(arg) { return this.a; // this的值取决于函数的调用方式}whatsThis(); // 'Global'whatsThis.call(obj); // 'Custom'whatsThis.apply(obj); // 'Custom' 当一个函数在其主体中使用 this 关键字时，可以通过使用函数继承自Function.prototype 的 call 或 apply 方法将 this 值绑定到调用中的特定对象。 12345678910111213function add(c, d) { return this.a + this.b + c + d;}var o = {a: 1, b: 3};// 第一个参数是作为‘this’使用的对象// 后续参数作为参数传递给函数调用add.call(o, 5, 7); // 1 + 3 + 5 + 7 = 16// 第一个参数也是作为‘this’使用的对象// 第二个参数是一个数组，数组里的元素用作函数调用中的参数add.apply(o, [10, 20]); // 1 + 3 + 10 + 20 = 34 使用 call 和 apply 函数的时候要注意，如果传递给 this 的值不是一个对象，JavaScript 会尝试使用内部 ToObject 操作将其转换为对象。因此，如果传递的值是一个原始值比如 7 或 ‘foo’，那么就会使用相关构造函数将它转换为对象，所以原始值 7 会被转换为对象，像 new Number(7) 这样，而字符串 ‘foo’ 转化成 new String(‘foo’) 这样，例如： 123456function bar() { console.log(Object.prototype.toString.call(this));}//原始值 7 被隐式转换为对象bar.call(7); // [object Number]","link":"/2018/06/19/JavaScript%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E2%80%94%E2%80%94this/"},{"title":"JavaScript笔记————正确使用分号","text":"JavaScript中分号的使用JavaScript中行末的分号表示语句结束，不过这个分号只有在单行内需要分割语句时才是必须的。然而，一些人认为在每个语句后面加分号是一种好的风格。这里为你提供了更多关于是否应该加分号的规则 — 查看 Your Guide to Semicolons in JavaScript 获取更多细节。 Required: When two statements are on the same line当在同一行多个语句(statement)时，各个语句之间分号才是必须的： 1234var i = 0; i++ // &lt;-- semicolon obligatory // (but optional before newline)var i = 0 // &lt;-- semicolon optional i++ // &lt;-- semicolon optional Optional: After statementsJavaScript中的分号用于分隔多个语句（不同于C，C++等一些语言，分号用于标示语句的结束），但如果语句后跟换行符（或者{block}中只有一个语句），则分号可以省略。语句（statement）是一段告诉计算机做某事的代码。以下是最常见的语句类型： 1234567var i; // variable declarationi = 5; // value assignmenti = i + 1; // value assignmenti++; // same as abovevar x = 9; // declaration &amp; assignmentvar fun = function() {...}; // var decl., assignmt, and func. defin.alert(&quot;hi&quot;); // function call 所有这些语句都可以以一个分号“;”作为结束，但并不是必须的。一些人认为用一个分号作为语句结束是一个好习惯——这使得对代码进行解析，压缩变得更加简单一点：不用担心去除换行符后，几条语句在同一行中不可分离。 Avoid! After a closing curly bracket(在一个结束的大括号之前) 不应该在一个结束的大括号”}”后面加分号。卫衣的例外是赋值语句，如var obj={};，参见上文。 12345678910// NO semicolons after }:if (...) {...} else {...}for (...) {...}while (...) {...}// BUT:do {...} while (...);// function statement: function (arg) { /*do this*/ } // NO semicolon after } After the round bracket of an if, for, while or switch statement(在，if，for，while或switch语句的圆括号之后)在if语句的{}之后加分号并不会有什么错误（这个分号会被忽略，并且可能在一些代码高亮插件中会提示一个警告，说明这是不必要的）。但是在不应该添加分号的地方（例如在if，for，while或switch语句的圆括号之后）加上分号是一个非常糟糕的主意：123456if (0 === 1); { alert(&quot;hi&quot;) }// equivalent to:if (0 === 1) /*do nothing*/ ;alert (&quot;hi&quot;); 上面的代码始终会弹出“hi”警示窗口，但是不是因为if中的条件满足。上面的写法JavaScript理解为if条件后面是一个空语句，而后面的语句并不属于if语句的范围。 Of course there’s an exception…一个重要的习惯：在for循环的（）中，分号仅在第一个和第二个语句之后，而不是在第三个之后： 12for (var i=0; i &lt; 10; i++) {/*actions*/} // correctfor (var i=0; i &lt; 10; i++;) {/*actions*/} // SyntaxError","link":"/2018/06/29/JavaScript%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%E5%88%86%E5%8F%B7/"},{"title":"JavaScript笔记——简介","text":"JavaScript简介历史起源JavaScript起源于Nombas公司开发的Cmm，一种嵌入式脚本语言，捆绑在CEnvi共享软件中。后来Nombas将其更名为ScriptEase。 诞生随着Internet的发展，客户端处理表单数据的难度加大，急需一种脚本语言来解决问题，那时正处于技术革新最前沿的 Netscape，开始认真考虑开发一种客户端脚本语言来解决简单的处理问题。 Netscape 的 Brendan Eich，开始着手为即将在 1995 年发行的 Netscape Navigator 2.0 开发一个称之为 LiveScript 的脚本语言，当时的目的是在浏览器和服务器（本来要叫它 LiveWire）端使用它。Netscape 与 Sun 及时完成 LiveScript 实现。 就在 Netscape Navigator 2.0 即将正式发布前，Netscape 将其更名为 JavaScript，目的是为了利用 Java 这个因特网时髦词汇。Netscape 的赌注最终得到回报，JavaScript 从此变成了因特网的必备组件。 混乱因为 JavaScript 1.0 如此成功，Netscape 在 Netscape Navigator 3.0 中发布了 1.1 版。恰巧那个时候，微软决定进军浏览器，发布了 IE 3.0 并搭载了一个 JavaScript 的克隆版，叫做 JScript（这样命名是为了避免与 Netscape 潜在的许可纠纷）。微软步入 Web 浏览器领域的这重要一步虽然令其声名狼藉，但也成为 JavaScript 语言发展过程中的重要一步。 在微软进入后，有 3 种不同的 JavaScript 版本同时存在：Netscape Navigator 3.0 中的 JavaScript、IE 中的 JScript 以及 CEnvi 中的 ScriptEase。与 C 和其他编程语言不同的是，JavaScript 并没有一个标准来统一其语法或特性，而这 3 种不同的版本恰恰突出了这个问题。随着业界担心的增加，这个语言的标准化显然已经势在必行。 标准化1997 年，JavaScript 1.1 作为一个草案提交给欧洲计算机制造商协会（ECMA）。第 39 技术委员会（TC39）被委派来“标准化一个通用、跨平台、中立于厂商的脚本语言的语法和语义”(http://www.ecma-international.org/memento/TC39.htm)。由来自 Netscape、Sun、微软、Borland 和其他一些对脚本编程感兴趣的公司的程序员组成的 TC39 锤炼出了 ECMA-262，该标准定义了名为 ECMAScript 的全新脚本语言。 在接下来的几年里，国际标准化组织及国际电工委员会（ISO/IEC）也采纳 ECMAScript 作为标准（ISO/IEC-16262）。从此，Web 浏览器就开始努力（虽然有着不同的程度的成功和失败）将 ECMAScript 作为 JavaScript 实现的基础。 ECMAScrip与JavaScript的关系JavaScript的语法遵循ECMAScrip标准，ECMAScrip是一种规范，而JavaScript只是这种规范的一种实现，比如ECMAScrip还有其它版本的实现——TypeScript。同时，JavaScript为了能在Web中处理多种问题，还实现了DOM和BOM。 DOMDOM（文档对象模型）是 HTML 和 XML 的应用程序接口（API）。DOM 将把整个页面规划成由节点层级构成的文档。HTML 或 XML 页面的每个部分都是一个节点的衍生物。DOM 通过创建树来表示文档，从而使开发者对文档的内容和结构具有空前的控制力。用 DOM API 可以轻松地删除、添加和替换节点。 BOMIE 3.0 和 Netscape Navigator 3.0 提供了一种特性 - BOM（浏览器对象模型），可以对浏览器窗口进行访问和操作。使用 BOM，开发者可以移动窗口、改变状态栏中的文本以及执行其他与页面内容不直接相关的动作。使 BOM 独树一帜且又常常令人怀疑的地方在于，它只是 JavaScript 的一个部分，没有任何相关的标准。 BOM 主要处理浏览器窗口和框架，不过通常浏览器特定的 JavaScript 扩展都被看做 BOM 的一部分。这些扩展包括： 弹出新的浏览器窗口 移动、关闭浏览器窗口以及调整窗口大小 提供 Web 浏览器详细信息的定位对象 提供用户屏幕分辨率详细信息的屏幕对象 对 cookie 的支持 IE 扩展了 BOM，加入了 ActiveXObject 类，可以通过 JavaScript 实例化 ActiveX 对象 由于没有相关的 BOM 标准，每种浏览器都有自己的 BOM 实现。有一些事实上的标准，如具有一个窗口对象和一个导航对象，不过每种浏览器可以为这些对象或其他对象定义自己的属性和方法。 JavaScript 引擎能够理解和执行 JavaScript 代码的程序或解释器。 同义词：JavaScript 解释器，JavaScript 的实现 JavaScript 引擎通常可以在 web 浏览器中被发现，包括 Chrome 中的 V8 ，火狐中的 SpiderMonkey ，以及 Edge 中的 Chakra 。每款引擎就像是一个用于其应用程序的语言模块，可以让其支持某种 JavaScript 语言的分支。JavaScript引擎的实现决定了JavaScript实现了ECMAScrip的哪些标准。 Reference http://developer.51cto.com/art/201711/557514.htm http://www.w3school.com.cn/js/pro_js_implement.asp","link":"/2018/05/30/JavaScript%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B/"},{"title":"JavaScript笔记————Arrow functions","text":"Arrow functions箭头函数表达式的语法比函数表达式更短，并且没有自己的this，arguments，super或 new.target。这些函数表达式更适用于那些本来需要匿名函数的地方，并且它们不能用作构造函数。 语法基础语法 12345678910(参数1, 参数2, …, 参数N) =&gt; { 函数声明 }(参数1, 参数2, …, 参数N) =&gt; 表达式（单一）//相当于：(参数1, 参数2, …, 参数N) =&gt;{ return 表达式; }// 当只有一个参数时，圆括号是可选的：(单一参数) =&gt; {函数声明}单一参数 =&gt; {函数声明}// 没有参数的函数应该写成一对圆括号。() =&gt; {函数声明} 高级语法 12345678910//加括号的函数体返回对象字面表达式：参数=&gt; ({foo: bar})//支持剩余参数和默认参数(参数1, 参数2, ...rest) =&gt; {函数声明}(参数1 = 默认值1,参数2, …, 参数N = 默认值N) =&gt; {函数声明}//同样支持参数列表解构let f = ([a, b] = [1, 2], {x: c} = {x: a + b}) =&gt; a + b + c;f(); // 6 描述引入箭头函数有两个方面的作用：更简短的函数并且不绑定this. 更短的函数12345678910111213141516var materials = [ 'Hydrogen', 'Helium', 'Lithium', 'Beryllium'];materials.map(function(material) { return material.length; }); // [8, 6, 7, 9]materials.map((material) =&gt; { return material.length;}); // [8, 6, 7, 9]materials.map(material =&gt; material.length); // [8, 6, 7, 9] 不绑定this在箭头函数出现之前，每个新定义的函数都有它自己的 this值（在构造函数的情况下是一个新对象，在严格模式的函数调用中为 undefined，如果该函数被称为“对象方法”则为基础对象等）。This被证明是令人厌烦的面向对象风格的编程。 123456789101112function Person() { // Person() 构造函数定义 `this`作为它自己的实例. this.age = 0; setInterval(function growUp() { // 在非严格模式, growUp()函数定义 `this`作为全局对象, // 与在 Person()构造函数中定义的 `this`并不相同. this.age++; }, 1000);}var p = new Person(); 在ECMAScript 3/5中，通过将this值分配给封闭的变量，可以解决this问题。 123456789function Person() { var that = this; that.age = 0; setInterval(function growUp() { // 回调引用的是`that`变量, 其值是预期的对象. that.age++; }, 1000);} 或者，可以创建绑定函数，以便将预先分配的this值传递到绑定的目标函数（上述示例中的growUp()函数）。 箭头函数不会创建自己的this,它从会从自己的作用域链的上一层继承this。因此，在下面的代码中，传递给setInterval的函数内的this与封闭函数中的this值相同： 123456789function Person(){ this.age = 0; setInterval(() =&gt; { this.age++; // |this| 正确地指向person 对象 }, 1000);}var p = new Person(); 与严格模式的关系鉴于 this 是词法层面上的，严格模式中与 this 相关的规则都将被忽略。 12345678910111213141516171819202122function Person() { this.age = 0; var closure = \"123\" setInterval(function growUp() { this.age++; console.log(closure) }, 1000);}var p = new Person();function PersonX() { 'use strict' this.age = 0; var closure = \"123\" setInterval(()=&gt;{ this.age++; console.log(closure) }, 1000);}var px = new PersonX(); 严格模式的其他规则依然不变. 通过 call 或 apply 调用由于 箭头函数没有自己的this指针，通过 call() 或 apply() 方法调用一个函数时，只能传递参数（不能绑定this—译者注），他们的第一个参数会被忽略。（这种现象对于bind方法同样成立—译者注） 1234567891011121314151617181920var adder = { base : 1, add : function(a) { var f = v =&gt; v + this.base; return f(a); }, addThruCall: function(a) { var f = v =&gt; v + this.base; var b = { base : 2 }; return f.call(b, a); }};console.log(adder.add(1)); // 输出 2console.log(adder.addThruCall(1)); // 仍然输出 2（而不是3 ——译者注） 不绑定arguments箭头函数不绑定Arguments 对象。因此，在本示例中，arguments只是引用了封闭作用域内的arguments： 1234567891011var arguments = [1, 2, 3];var arr = () =&gt; arguments[0];arr(); // 1function foo(n) { var f = () =&gt; arguments[0] + n; // 隐式绑定 foo 函数的 arguments 对象. arguments[0] 是 n return f();}foo(1); // 2","link":"/2018/06/19/JavaScript%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E2%80%94%E2%80%94Arrow-functions/"},{"title":"Mybatis Note","text":"Myabtis相关知识点Mybatis缓存Mybatis本身具有两级缓存:本地缓存和二级缓存.每当MyBatis创建一个Session时,会自动分配一个本地缓存绑定到Session.在一个Session中的查询操作会被保存到本地缓存,于是当相同的查询语句和相同的参数时,不会直接从数据库中读取数据,而是直接从缓存中获取. 非常重要的时,如果本地缓存的作用域是SESSION,MyBatis返回的结果是缓存中对象的引用.当对返回的结果进行修改时,缓存中的数据也会被修改. 1234567891011SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build( Resources.getResourceAsStream(App.class.getClassLoader(), \"mybatis-config.xml\"));try (SqlSession session = sqlSessionFactory.openSession()) { AppInfoMapper mapper = session.getMapper(AppInfoMapper.class); AppInfoPo po = mapper.selectById(1); System.out.println(po); po.setAppName(\"modified\"); po = mapper.selectById(1); System.out.println(po);} 如上面的代码,两次输出是不一样的;而下面的代码输出结果一致: 123456789101112SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build( Resources.getResourceAsStream(App.class.getClassLoader(), \"mybatis-config.xml\"));try (SqlSession session = sqlSessionFactory.openSession()) { AppInfoMapper mapper = session.getMapper(AppInfoMapper.class); AppInfoPo po = mapper.selectById(1); System.out.println(po); po.setAppName(\"modified\"); session.clearCache(); po = mapper.selectById(1); System.out.println(po);} JDBC最初接触JDBC时,获取一个数据库的连接方式应该是这个样子的: 1234// 加载驱动Class.forName(\"com.mysql.jdbc.Driver\")// 获取连接DriverManager.getConnection(\"jdbc:mysql://127.0.0.1:3306/imooc\", \"root\", \"root\"); 但是在JDBC2.0中,添加了javax.sql.DataSource接口,通过使用这个接口,更加方便获取数据库连接.Mybatis的PooledDataSource和UnpooledDataSource都实现了这个接口.第三方的数据库连接池都实现了这个借口,所以在Mybatis和Spring整合的时候,我们可以配置第三方的数据源管理工具来提供数据库的连接.","link":"/2019/08/29/Mybatis-Note/"},{"title":"Node.js 模块系统","text":"Node.js 模块系统在Node.js模块系统中，每一个文件都被视为一个分开的模块。例如，考虑一个名为 foo.js 的文件: 12const circle = require('./circle.js');console.log(`The area of a circle of radius 4 is ${circle.area(4)}`); 在第一行， foo.js 加载 circle.js 模块，该模块和 foo.js 在同一目录下。 circle.js 的内容如下: 12345const { PI } = Math;exports.area = (r) =&gt; PI * r ** 2;exports.circumference = (r) =&gt; 2 * PI * r; 模块 circle.js 将函数 area() 和 circumference() 导出。函数和对象被添加到模块的root，通过将其赋值到 exports 这个特殊的对象的额外属性中。 模块内的本地变量对于其它模块是私有的，因为模块会被自动包含在一个函数中。上面的例子中，变量 PI 是 circle.js 私有的。 属性 module.exports 可以被重新赋予一个新的值（可以是函数或者对象）。下面的代码中， bar.js 使用了导出 Square 类的 square 模块： 123const Square = require('./square.js');const mySquare = new Square(2);console.log(`The area of mySquare is ${mySquare.area()}`); square 模块在 square.js 中定义： 12345678910// assigning to exports will not modify module, must use module.exportsmodule.exports = class Square { constructor(width) { this.width = width; } area() { return this.width ** 2; }}; 模块系统的实现在 require(‘module’) 模块中。 require.resolve(request[, options])使用 require() 相同的内部机制，寻找模块的位置，但是不会加载模块，而是仅仅返回对应的文件名。 exports shortcut需要注意的是，不要直接对 exports 直接赋值，考虑到浅拷贝，如果对 exports 重新赋值，那么它将不会被绑定到 module.exports: 12module.exports.hello = true; // Exported from require of moduleexports = { hello: false }; // Not exported, only available in the module 当 module.exports 属性被一个新对象完全替换时，通常也会重新分配导出： 123module.exports = exports = function Constructor() { // ... etc.};","link":"/2018/06/19/Node-js-%E6%A8%A1%E5%9D%97%E7%B3%BB%E7%BB%9F/"},{"title":"OpenWAF——系统过滤规则","text":"OpenWAF系统过滤规则 ID Name Phase Severity Description Action 300002 malicious.trojan.general access high 检测访问木马页面，检查请求头名称中含有关键字的HTTP请求。请求头key值是否以字符串”x_key”或者”x_file”结束，忽略大小写。 deny 300003 malicious.trojan.general.a access high 检测木马访问，检查请求文件名中是否含有关键字。relative request URL（相对请求路径）中包含关键字”rooot%.exe”，忽略大小写。 deny 300004 malicious.webshell access critical 检测向常见静态资源文件发出的HTTP POST请求。正则匹配请求url中是否以常见静态资源文件的扩展名作为结束（gif, jpe?g, png, bmp, js, css, txt, exe, docx?, xlsx?, pptx?, zip, rar, 7z）忽略大小写。 deny 300005 malicious.webshell.a header_filter critical 检测向常用图片格式文件请求但返回text类型的请求(gif, jpe?g, png, bmp) 忽略大小写 deny 300006 malicious.webshell.b access critical 检测对设备文件名的请求 deny 700001 auto.crawler.general access medium 检测网站爬取工具，检查User-Agent请求头的值是否含有关键字，忽略大小写 deny 700002 auto.crawler.general.a access medium 检测网站爬取工具，检查User-Agent请求头的值是否含有关键字 deny 700003 auto.crawler.general.b access medium 检测网站爬取工具，检查User-Agent请求头的值是否含有关键字 deny 700004 auto.scanner.appscan access medium 检测网站扫描工具，检查请求头的值是否含有关键字 deny 700006 auto.scanner.general.a access medium 检测网站扫描工具，检查User-Agent的值含有关键字的HTTP请求 deny 700007 auto.scanner.general.b access medium 检测网站扫描工具，检查User-Agent的值含有关键字的HTTP请求 deny 700008 auto.scanner.nessus access medium 检测网站扫描工具，检查User-Agent的值含有关键字的HTTP请求 deny 700009 auto.scanner.wvs access high 检测网站扫描工具，检查请求头的值含有关键字的HTTP请求 deny 700010 auto.scanner.general.c access medium 检测网站扫描工具，检查User-Agent的值含有关键字的HTTP请求 deny 700011 auto.crawler.general.c access medium 检测网站扫描工具，检查User-Agent的值含有关键字的HTTP请求 deny 700012 auto.scanner.wvs.a access medium 检测网站扫描工具，检查请求头的值含有关键字的HTTP请求 deny 700014 auto.scanner.appscan.a access medium 检测网站扫描工具，检查请求头的值是否含有关键字 deny 700015 auto.scanner.appscan.b access medium 检测网站扫描工具，检查请求头的值是否含有关键字 deny 700017 auto.scanner.general.e access medium 检测网站扫描工具，检查请求头及请求头的值 deny 700018 auto.scanner.general.f access medium 检测网站扫描工具，检查请求URI deny 100022 attack.commentSpam access medium 检测垃圾信息，检查User-Agent请求头的值为常见垃圾广告发送者的HTTP请求 deny 100023 attack.commentSpam.a access medium 检测垃圾信息，检查参数中以http:开头的HTTP请求 pass 100024 attack.commentSpam.b access medium 检测垃圾信息，检查参数中含有4个及以上http:/的HTTP请求 pass 100025 attack.dirTraversal access high 检测路径遍历攻击，检查请求URI、请求头、请求体 deny 100026 attack.dirTraversal.a access high 检测路径遍历攻击，检查请求文件名、参数 deny 100087 attack.injection.cf access critical 检测针对ColdFusion服务器的注入攻击，检查Cookie、参数 deny 100088 attack.injection.cf.a access critical 检测针对ColdFusion服务器的注入攻击，检查请求URI、请求头、请求体 deny 100205 attack.injection.ldap access critical 检测LDAP注入攻击，检查Cookie、参数 deny 100206 attack.injection.ldap.a access critical 检测LDAP注入攻击，检查请求URI、请求头、请求体 deny 100207 attack.injection.osCmd access critical 检测命令注入攻击，检查Cookie、参数 pass 200001 attack.injection.sql.libinjection access critical sqli防护 deny 200002 attack.xss.libinjection access high xss防护 deny","link":"/2018/06/29/OpenWAF%E2%80%94%E2%80%94%E7%B3%BB%E7%BB%9F%E8%BF%87%E6%BB%A4%E8%A7%84%E5%88%99/"},{"title":"Spring入门","text":"Spring入门记录学习《Spring实战》过程中的笔记 第一个项目使用maven管理依赖包，创建一个简单的项目，目录结构如下： 12345678910.├── pom.xml├── src│ ├── main│ └── test└── target ├── classes ├── generated-sources ├── generated-test-sources └── test-classes pom.xml中添加以下依赖项： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;LATEST&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;LATEST&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;LATEST&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;LATEST&lt;/version&gt;&lt;/dependency&gt; 添加一个CompactDisc接口，接口包含一个play方法： 123public interface CompactDisc { void play();} 添加一个类，实现CompactDisc接口： 12345678@Componentpublic class SgtPeppers implements CompactDisc { private String title = \"Sgt. Pepper's Lonely Hearts ClubBand\"; private String artist = \"The Beatles\"; public void play() { System.out.println(\"Playing \"+title+\" by \"+artist); }} 为该类添加了 @Component 注解，表明该类会作为组件类，并告知Spring要为这个类创建bean，这样就没有必要显式配置SgtPeppersbean。 注意的是，组件扫描在默认情况下是不启用的。还是需要显式配置一下Spring，从而命令Spring去寻找带有 @Component 注解的类，并为其创建bean。 创建配置类： 1234@Configuration@ComponentScanpublic class CDPlayerConfig {} 类CDPlayerConfig通过Java代码定义了Spring的装配规则。这里，在类里面没有显式地声明任何bean，而是使用注解 @ComponentScan ，这个注解能够在Spring中启用组件扫描。在没有额外配置的情况下， @ComponentScan注解会扫描与配置类相同的包。 创建测试 这里创建一个JUnit测试，代码如下： 123456789101112@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = CDPlayerConfig.class)public class SgtPeppersTest { @Autowired private CompactDisc cd; @Test public void play() { cd.play(); assertNotNull(cd); }} 注解 @RunWith注解让测试开始的时候自动创建Spring应用上下文。注解 @ContextConfiguration告诉测试类从哪里加在配置。 通过Java代码装配bean在我们的第一个项目中，spring上下文的配置没有使用xml，并且创建的SpringConfig类没有实际的实现代码配置。而是使用的组件扫描和自动装配。下面尝试在SpringConfig类中利用java代码进行显式配置。 创建配置类使用之前创建的CDPlayerConfig类，移除 @ComponentScan注解。 声明简单的bean在CDPlayerConfig类中添加一个方法用于创建所需类型的实例，然后给这个方法添加 @Bean 注解。下面的代码声明了CompactDisc bean: 1234@Beanpublic CompactDisc sgtPeppers(){ return new SgtPeppers();} @Bean注解会告诉Spring这个方法将返回一个对象，该对象要注册为Spring应用上下文中的bean。方法体中包含了最终产生bean实例的逻辑。 默认情况下，bean的ID与带有@Bean注解的方法名是一样的。在本例中，bean的名字将会是sgtPeppers。可以通过指定 name 属性给bean设置不同的名字： 1234@Bean(name=\"customname\")public CompactDisc sgtPeppers(){ return new SgtPeppers();} 在方法的逻辑中，不管通过何种方式实现，只要最终能够生成一个CompactDisc实例即可。 借助JavaConfig实现注入在JavaConfig中装配bean的最简单方式就是引用创建bean的方法。例如，下面就是一种声明CDPlayer的可行方案： 1234@Beanpublic MediaPlayer cdPlayer(){ return new CDPlayer(sgtPeppers());// bean 默认是单例的，这里创建CDPlayerbean时，不会真的调用sgtPeppers。} 在cdPlayer()方法中，看起来似乎是通过调用sgtPeppers()方法得到的CompactDisc实例，其实由于sgtPeppers()方法加上了@Bean注解，Spring会拦截所有对该方法的调用，并且保证直接返回该方法所创建的bean，而不是每次都对其进行实际的调用。 还有一种更简单的装配方式： 1234@Beanpublic MediaPlayer cdPlayer(CompactDisc cd){ return new CDPlayer(cd);} 通过上面的方式引用其他的bean通常是最佳的选择，因为不要求将CompactDisc声明到同一个配置类中。在这里甚至没有要求CompactDisc必须要在JavaConfig中声明，实际上它可以通过组件扫描功能自定发现或者通过XML来进行配置。可以将配置分散到多个配置类，XML文件以及自动扫描和装配bean之中，只要功能完整健全即可。","link":"/2018/07/26/Spring%E5%85%A5%E9%97%A8/"},{"title":"TensorFlow API","text":"TensorFlow API学习笔记TF-Slimhttps://www.2cto.com/kf/201706/649266.html NHWC, NCHW数据格式区别 NHWC：[batch, in_height, in_width, in_channels] 一般在CPU模式下使用该数据格式 NCWH：[batch, in_channels, in_height, in_width] 一般在GPU模式下使用该数据格式 转换12345678910111213141516# NHWC –&gt; NCHW：import tensorflow as tfx = tf.reshape(tf.range(24), [1, 3, 4, 2])out = tf.transpose(x, [0, 3, 1, 2])print x.shapeprint out.shape# NCHW –&gt; NHWC：import tensorflow as tfx = tf.reshape(tf.range(24), [1, 2, 3, 4])out = tf.transpose(x, [0, 2, 3, 1])print x.shapeprint out.shape TensorFlow填充张量函数：tf.pad1234567pad （ tensor ， paddings ， mode = 'CONSTANT' ， name = None ， constant_values = 0 ） 填充张量。此操作根据您指定的 paddings 来填充一个 tensor。paddings 是一个具有形状 [n, 2] 的整数张量，其中 n 是 tensor 的秩。对于每个输入维度 D，paddings [D, 0] 表示在该维度的 tensor 内容之前要添加多少个值，而 paddings[D, 1] 表示在该维度中的 tensor 内容之后要添加多少值。如果 mode 是 “REFLECT”，那么这两个paddings[D, 0] 和 paddings[D, 1] 不得大于 tensor.dim_size(D) - 1。如果 mode 是 “SYMMETRIC”，那么这两个 paddings[D, 0] 和 paddings[D, 1] 不得大于tensor.dim_size(D)。 例如： 123456789101112131415161718# 't' is [[1, 2, 3], [4, 5, 6]].# 'paddings' is [[1, 1,], [2, 2]].# 'constant_values' is 0.# rank of 't' is 2.pad(t, paddings, \"CONSTANT\") ==&gt; [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0], [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]]pad(t, paddings, \"REFLECT\") ==&gt; [[6, 5, 4, 5, 6, 5, 4], [3, 2, 1, 2, 3, 2, 1], [6, 5, 4, 5, 6, 5, 4], [3, 2, 1, 2, 3, 2, 1]]pad(t, paddings, \"SYMMETRIC\") ==&gt; [[2, 1, 1, 2, 3, 3, 2], [2, 1, 1, 2, 3, 3, 2], [5, 4, 4, 5, 6, 6, 5], [5, 4, 4, 5, 6, 6, 5]] 参数： tensor：张量。 paddings：int32 类型的张量。 mode：取值为 “CONSTANT”、”REFLECT” 或 “SYMMETRIC”（不区分大小写） name：操作的名称（可选）。 constant_values：在 “CONSTANT” 模式下，要使用的标量填充值，必须与 tensor 具有相同类型。返回： 该函数返回一个张量，与 tensor 具有相同的类型。 可能引发的异常： ValueError：模式不是 “CONSTANT”、”REFLECT” 或 “SYMMETRIC” 中的一种时。","link":"/2018/06/07/TensorFlow-API/"},{"title":"Ubuntu 16.04 or later 版本添加Tomcat为服务","text":"Ubuntu16.04 or later版本中Tomcat的安装准备工作Step 1：JDK安装和Tomcat安装基本要求不用多说，需要下载适合Tomcat版本的JDK。具体JDK的安装方法可自行移步搜索引擎查找方法。然后便是下载合适版本的Tomcat，解压到合适的目录。记住自己JDK安装目录，和Tomcat存放目录。笔者的JDK版本为1.8.0_172, Tomcat版本为9.0。JDK目录为 ~/DevelopTools/jdk1.8.0_172，Tomcat目录为 /opt/tomcat9。 Step 2: 创建Tomcat用户出于安全考虑，Tomcat应该作为非特权用户运行（即不是root用户）。我们将创建一个将运行Tomcat服务的新用户和组。 首先创建一个新的 tomcat 用户组： 1sudo groupadd tomcat 然后创建名为 tomcat 的用户，并把该用户添加到刚刚创建的 tomcat 用户组当中。设置用户的home目录为tomcat安装目录，这里笔者设置为 /opt/tomcat9。最后需要设置shell为 /bin/false 防止 tomcat 用户获取shell。 12# notice：change the dir according to your installation structuresudo useradd -s /bin/false -g tomcat -d /opt/tomcat9 tomcat Step 3: 修改文件夹权限我们设置的tomcat用户需要能够访问Tomcat安装目录。切换到我们解压缩Tomcat安装的目录，将tomcat安装目录所属用户组设置为 tomcat 用户组： 12# notice：change the dir according to your installation structuresudo chgrp -R tomcat /opt/tomcat9 修改 conf 目录中文件的权限，使得tomcat用户组有可读权限，并且修改 conf 权限使得tomcat组有可执行权限： 1234# notice：change the dir according to your installation structurecd /opt/tomcat9sudo chmod -R g+r confsudo chmod g+x conf 最后还需要修改 webapps, work, temp, 和 logs 目录的所有者为tomcat： 123# notice：change the dir according to your installation structurecd /opt/tomcat9sudo chown -R tomcat webapps/ work/ temp/ logs/ Step 4: 创建系统服务文件我们希望能够将Tomcat作为服务运行，因此我们将设置systemd服务文件。在 /etc/systemd/system目录下创建名为 tomcat.service的文件： 1sudo vim /etc/systemd/system/tomcat.service 将以下内容粘贴到 tomcat.service 文件中。如有必要，修改JAVA_HOME的值以匹配您在系统上找到的值。您可能还想修改CATALINA_OPTS中指定的内存分配设置： 12345678910111213141516171819202122232425[Unit]Description=Apache Tomcat Web Application ContainerAfter=network.target[Service]Type=forkingEnvironment=JAVA_HOME=/home/fangkui/DevelopTools/jdk1.8.0_172/jreEnvironment=CATALINA_PID=/opt/tomcat9/temp/tomcat.pidEnvironment=CATALINA_HOME=/opt/tomcat9Environment=CATALINA_BASE=/opt/tomcat9Environment='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'Environment='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'ExecStart=/opt/tomcat/bin/startup.shExecStop=/opt/tomcat/bin/shutdown.shUser=tomcatGroup=tomcatUMask=0007RestartSec=10Restart=always[Install]WantedBy=multi-user.target 然后保存文件。 接下来，重新加载systemd守护程序，以便它知道我们的服务文件： 1sudo systemctl daemon-reload 键入以下命令启动Tomcat服务： 1sudo systemctl start tomcat 键入以下内容，仔细检查它是否正常启动： 1sudo systemctl status tomcat 正常执行结果如下： 123456789101112● tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2018-08-27 13:28:03 CST; 1h 4min ago Process: 1485 ExecStart=/opt/tomcat9/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 1629 (java) Tasks: 49 (limit: 4915) CGroup: /system.slice/tomcat.service └─1629 /home/fangkui/DevelopTools/jdk1.8.0_172/jre/bin/java -Djava.util.logging.config.file=/opt/tomcat9/c8月 27 13:28:02 fangkui-linux systemd[1]: Starting Apache Tomcat Web Application Container...8月 27 13:28:03 fangkui-linux systemd[1]: Started Apache Tomcat Web Application Container.lines 1-11/11 (END) 可以看到服务中的 active (running)字样 如果启动无误，就可以将服务添加到自动启动中了： 1sudo systemctl enable tomcat Reference https://www.digitalocean.com/community/tutorials/how-to-install-apache-tomcat-8-on-ubuntu-16-04https://devops.profitbricks.com/tutorials/how-to-install-and-configure-tomcat-8-on-ubuntu-1604/","link":"/2018/08/27/Ubuntu-16-04-or-later-%E7%89%88%E6%9C%AC%E6%B7%BB%E5%8A%A0Tomcat%E4%B8%BA%E6%9C%8D%E5%8A%A1/"},{"title":"WAF攻防","text":"WAF攻防架构层绕过WAF寻找源站当前多数云WAF架构，例如百度云加速、360安全卫士等，通过更改DNS解析，把流量引入WAF集群，流量经过检测后转发请求到源站。如图，liusscs.com接入接入WAF后，liusscs.comd的DNS解析结果指向WAF集群，用户的请求将发送给WAF集群，WAF集群经过检测认为非攻击请求再转发给源站。从云WAF架构考虑，如果HTTP请求都没有经过WAF集群直接到达源站，顺理成章bypass WAF。所以关键在于发现源站的IP地址。常用方法如下，可能还有很多很多思路： 信息泄露发现源站IP。信息泄露的途径很多，细心留言往往能发现。常用的方法如下： 网站页面注销是否包含源站IP。 GIHUB源代码泄露是否包含源站IP。 未接入WAF前，真实IP地址是否被搜索引擎等服务收录。 穷举IP地址，根据特征发现服务器真实IP地址。对于国内的服务器，穷举国内的IP，访问每个IP的HTTP服务，根据页面特征检测响应页面，判断IP是否为源站IP地址。曾经乌云有人分享过，完成一次国内IP扫描只需要8-9小时，可是现在找不到那篇文章。 利用同网段一些在云服务商的站点，同时使用云服务商提供的WAF服务。当流量不是通过DNS解析引流到WAF，流量必须经过WAF的检测，这是不能通过发行源站进行绕过。可以考虑在云服务商买一台VPS，通过VPS攻击目标站点，因为流量是局域网，可能不经过WAF检测，实现绕过。能不能成功，关键在于云服务商的网络配置。 利用边界漏洞如果未能发现源站IP，可以尝试寻找子站的SSRF漏洞。如果子站访问目标站不经过WAF集群，可以利用SSRF漏洞来绕过WAF。 资源限制角度绕过WAF这是众所周知、而又难以解决的问题。如果HTTP请求POST BODY太大，检测所有的内容，WAF集群消耗太大的CPU、内存资源。因此许多WAF只检测前面的几K字节、1M、或2M。对于攻击者而然，只需要在POST BODY前面添加许多无用数据，把攻击payload放在最后即可绕过WAF检测。 协议层面绕过WAF的检测即使流量都确保经过WAF，如果WAF的防御策略根本就没有检测payload，那么也就能绕过WAF。协议层面绕过WAF，利用WAF解析协议的问题，使得payload被认为不是请求的HTTP请求的内容。从经验总结出WAF解析协议的常出现问题的三个方向。 协议覆盖不全。 协议解析不正确。 协议解析与后WEB容器的协议解析不一致。","link":"/2018/07/24/WAF%E6%94%BB%E9%98%B2/"},{"title":"Web Development——WebSocket","text":"WebSocketWebSocket是HTML5开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。 在WebSocket API中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。 浏览器通过 JavaScript 向服务器发出建立 WebSocket 连接的请求，连接建立以后，客户端和服务器端就可以通过 TCP 连接直接交换数据。 当你获取 Web Socket 连接后，你可以通过 send() 方法来向服务器发送数据，并通过 onmessage 事件来接收服务器返回的数据。 以下 API 用于创建 WebSocket 对象： 1var Socket = new WebSocket(url, [protocol] ); 以上代码中的第一个参数 url, 指定连接的 URL。第二个参数 protocol 是可选的，指定了可接受的子协议。 WebSocket 实例WebSocket 协议本质上是一个基于 TCP 的协议。 为了建立一个 WebSocket 连接，客户端浏览器首先要向服务器发起一个 HTTP 请求，这个请求和通常的 HTTP 请求不同，包含了一些附加头信息，其中附加头信息”Upgrade: WebSocket”表明这是一个申请协议升级的 HTTP 请求，服务器端解析这些附加的头信息然后产生应答信息返回给客户端，客户端和服务器端的 WebSocket 连接就建立起来了，双方就可以通过这个连接通道自由的传递信息，并且这个连接会持续存在直到客户端或者服务器端的某一方主动的关闭连接。 客户端的 HTML 和 JavaScript目前大部分浏览器支持 WebSocket() 接口，你可以在以下浏览器中尝试实例： Chrome, Mozilla, Opera 和 Safari。 runoob_websocket.html 文件内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;script type=\"text/javascript\"&gt; function WebSocketTest() { if (\"WebSocket\" in window) { console.log(\"您的浏览器支持 WebSocket!\"); // 打开一个 web socket var ws = new WebSocket(\"ws://localhost:8080/\"); ws.onopen = function () { // Web Socket 已连接上，使用 send() 方法发送数据 ws.send(\"客户端发送数据\"); console.log(\"数据发送中&gt;&gt;&gt;\"); }; ws.onmessage = function (evt) { var received_msg = evt.data; console.log( evt) console.log(\"数据接受成功....\") }; ws.onclose = function () { // 关闭 websocket console.log(\"连接已关闭...\"); }; } else { // 浏览器不支持 WebSocket console.log(\"您的浏览器不支持 WebSocket!\"); } } &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"sse\"&gt; &lt;a href=\"javascript:WebSocketTest()\"&gt;运行 WebSocket&lt;/a&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; WebSocket 服务端这里可以借助node.js的 nodejs-websocket npm包搭建服务。 1234567891011121314var ws = require(\"nodejs-websocket\")// Scream server example: \"hi\" -&gt; \"HI!!!\"var server = ws.createServer(function (conn) { console.log(\"New connection\") conn.on(\"text\", function (str) { console.log(\"Received \"+str) conn.sendText(str.toUpperCase()+\"!!!\") conn.sendText(\"Second phase\") }) conn.on(\"close\", function (code, reason) { console.log(\"Connection closed\") })}).listen(8080) Referncehttp://www.runoob.com/html/html5-websocket.html","link":"/2018/07/24/Web-Development%E2%80%94%E2%80%94WebSocket/"},{"title":"spring boot profile与maven profile整合","text":"Maven Profile与Spring Profile的整合动机往往一个项目在开发、QA、生产环境的配置是不同的。开始接触maven管理项目时，没有使用过profile。所以自己开发的第一个项目在开发和部署的时候通过手动修改所有配置，比较麻烦。导致我打包部署后，无法再进行开发环境的测试。 配置过程Spring Profile配置由于最新配置文件推荐使用yml格式，所以笔者创建的上下文配置文件如下： application.yml spring 主配置文件，不管激活那个profile都会被读取并生效，需要在这个配置文件里面设置激活的profile； application-dev.yml 开发环境配置文件，命名方式必须为application-{profilename}.yml，不然无法辨别属于哪个profile的配置文件； application-prod.yml 生产环境配置文件。 application.yml 文件中必须配置需要激活的profile名称，配置方式如下： 123spring: profiles: active: dev maven配置 在pom.xml中添加profiles，配置方式如下： 123456789101112&lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;!--profile id，用于在maven命令行中唯一标识一个profile配置项--&gt; &lt;properties&gt; &lt;!--profile激活后，各个参数对应的值，可以在资源文件导入一下配置的properties--&gt; &lt;profileActive&gt;dev&lt;/profileActive&gt; &lt;quartz.URL&gt;×××&lt;/quartz.URL&gt; &lt;quartz.user&gt;×××&lt;/quartz.user&gt; &lt;qurtz.password&gt;×××&lt;/qurtz.password&gt; &lt;/properties&gt; &lt;activation&gt; &lt;!--指定默认激活--&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt;&lt;/profile&gt; 添加资源过滤，控制激活不同profile之后，哪些资源文件是必要加载的和没必要加载的： 1234567891011121314151617&lt;resources&gt; &lt;resource&gt;&lt;!--资源路径--&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;!--是否需要额外处理--&gt; &lt;excludes&gt; &lt;!--排除部分资源文件--&gt; &lt;exclude&gt;application-dev.yml&lt;/exclude&gt; &lt;exclude&gt;application-prod.yml&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;!--加载必要资源文件--&gt; &lt;include&gt;application-${profileActive}.yml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt;&lt;/resources&gt; 在pom.xml中指定profile中的properties属性读取方式，需要添加插件 maven-resources-plugin，具体配置如下： 123456789&lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;configuration&gt; &lt;delimiters&gt; &lt;!--指定读取properties的特殊标志符号--&gt; &lt;delimiter&gt;@&lt;/delimiter&gt; &lt;/delimiters&gt; &lt;/configuration&gt;&lt;/plugin&gt; 通过以上配置，就可在其他作为资源文件的配置文件中读取properties中配置的属性，具体做法为在需要的地方添加 @property@。 修改application.yml在前面的配置中，我们直接硬编码指定了spring激活的profile。通过maven profile中的properties设置，我们可以直接用maven进行控制激活的profile。只需要修改激活profile的参数值为@profileActive@，修改后的application.yml如下： 123spring:profiles: active: @profileActive@小结通过maven的profile中properties属性，我们还可以直接控制log4j中的配置，只需要通过引入参数就行了。 Reference https://blog.csdn.net/lihe2008125/article/details/50443491","link":"/2018/08/20/spring-boot-profile%E4%B8%8Emaven-profile%E6%95%B4%E5%90%88/"},{"title":"Openssl Cook Book","text":"Openssl使用解析秘钥生成可以使用openssl生成非对称加密秘钥，支持的加密算法包括 RSA,DSA和ECDSA。可使用 genrsa 命令生成RSA私钥： 1openssl genrsa -aes128 -out fd.key 2048 命令输出内容如下： 123456Generating RSA private key, 2048 bit long modulus........................................................................................................................................................+++.....................+++e is 65537 (0x10001)Enter pass phrase for fd.key:Verifying - Enter pass phrase for fd.key: 上面的命令指定了使用AES-128算法对RSA私钥进行加密。还可以指定其他加密算法，AES-192,AES-256. 生成的私钥以PEM格式保存到文件中，查看内容大致如下： 12345678910-----BEGIN RSA PRIVATE KEY-----Proc-Type: 4,ENCRYPTEDDEK-Info: AES-128-CBC,5E99F130761332EC98894B9AA52850501Io5u8+FN5ab67tlWjnZnspdWcgjQeMJYyE6QR2QKmFyUg7pFl8ukh+wtqhFInP5hv4xmJpJd9sTAr9cT5abD14XtEyj1kBUUx5NZvyJ9bfkyCaoXkL6SG6VSY+cEuxa[21 lines removes...]9U2k9ZAdumOKlyEYk2dKkc3c1uC7FuPwNvGsOhJmFfOPqCPVWz7wRG4z4S++jGu4/tY5xJgUFZP8oKjfHAKfxReoeIA+ULiWaUNTXWydP3jjDKgy5zJ+mI1wT8QBreO3-----END RSA PRIVATE KEY----- 我们可以通过使用 rsa命令查看私钥的详细结构： 1234567891011121314151617181920212223242526272829➜ openssl openssl rsa -text -in fd.keyEnter pass phrase for fd.key:Private-Key: (2048 bit)modulus: 00:eb:33:e5:5f:09:38:f9:12:3f:0f:dc:9d:69:16: [...]publicExponent: 65537 (0x10001)privateExponent: 00:c9:c7:9b:e0:0c:69:74:44:d3:15:28:8f:62:b8: [...]prime1: 00:f9:d6:01:2e:11:c3:ca:fd:22:92:66:c6:7b:df: [...]prime2: 00:f1:01:77:4c:03:51:44:31:4a:c8:90:24:7f:c2: [...]exponent1: 00:e5:4a:d7:c2:64:63:81:c5:59:19:6c:61:f6:06: [...]exponent2: 39:53:b4:f1:d2:d4:30:d9:5c:c5:8a:6b:f4:2b:d6: [...]coefficient: 67:b4:15:f4:ba:ef:57:bf:95:17:2a:e0:35:8c:d2: [...]writing RSA key-----BEGIN RSA PRIVATE KEY-----[...]-----END RSA PRIVATE KEY----- 我们可以使用 rsa命令生成私钥对应的公钥以单独保存： 123➜ openssl openssl rsa -in fd.key -pubout -out fd_public.keyEnter pass phrase for fd.key:writing RSA key 原始内容结构如下: 12345678910➜ openssl cat fd_public.key-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA6zPlXwk4+RI/D9ydaRbtNfb7xB1765vsAd5XOwFi3+oqAQnkmyea5h4lm5C53xcve+U/SXDiZaijxx2meTeQUz7hN5eUgqWgnGexFYMER9WAqzr4VGoesz4NNo8snHLlw7osaWHxvqO930JElb5gpRYs+H0L9Xy1vA6Zg6LBxj89xPEhttfmkWxL2FTBsL5hzlYjYJDmLy/MJsUbmoNs8WncX9ItG3CZb8FBhVVcmbXravJ/HDEjJvonkdJZa1iBkJSy0DCeeLbeCwXY6uf+83IQwzs70uL3I4qRQg4CyzhLUAt6EDsIdgTkjJv0mtFnbTV6kVswRQFymMu6I18LhwIDAQAB-----END PUBLIC KEY----- 创建CA签名请求当我们生成了私钥之后，可以用私钥继续创建Certification Signing Request（CSR）。这是一个要求CA签署证书的正式请求，它包含请求证书的实体的公钥和有关该实体的一些信息。这些数据都将成为证书的一部分。 CSR会被与起承载的公钥（实体的公钥）对应的私钥加密。 CSR创建通常是一个交互式过程，在此过程中您将提供证书专有名称的元素. 123456789101112131415161718192021➜ openssl openssl req -new -key fd.key -out fd.csrEnter pass phrase for fd.key:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:JSLocality Name (eg, city) []:SZOrganization Name (eg, company) [Internet Widgits Pty Ltd]:USTCOrganizational Unit Name (eg, section) []:IT DepartmentCommon Name (e.g. server FQDN or YOUR name) []:www.test.comEmail Address []:fangkui@ustc.edu.cnPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []: 生成请求成功后，我们可以查看请求具体信息： 12345678910111213141516171819➜ openssl openssl req -text -in fd.csr -nooutCertificate Request: Data: Version: 0 (0x0) Subject: C=CN, ST=JS, L=SZ, O=USTC, OU=IT Department, CN=www.test.com/emailAddress=fangkui@ustc.edu.cn Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:eb:33:e5:5f:09:38:f9:12:3f:0f:dc:9d:69:16: [16 more lines ...] 0b:87 Exponent: 65537 (0x10001) Attributes: a0:00 Signature Algorithm: sha256WithRSAEncryption 23:29:3f:65:68:f7:f2:3d:4d:c1:98:f4:96:ac:5c:f9:da:45: [13 more lines...] f1:5f:72:ce 如果您要续订证书并且不想对其中显示的信息进行任何更改，则可以为自己节省一些输入。 使用以下命令，您可以从现有证书创建全新的CSR： 1openssl x509 -x509toreq -in fd.crt -out fd.csr -signkey fd.key 无人值守的CSR生成可以提供一个自定义的配置文件来替代交互式的CSR创建过程。比如创建一个如下内容的配置文件 fd.cnf: 12345678910111213[req]prompt = nodistinguished_name = dnreq_extensions = extinput_password = PASSPHRASE[dn]CN = www.feistyduck.comemailAddress = webmaster@feistyduck.comO = Feisty Duck LtdL = LondonC = GB[ext]subjectAltName = DNS:www.feistyduck.com,DNS:feistyduck.com 然后可以直接创建CSR： 1openssl req -new -config fd.cnf -key fd.key -out fd.csr 自签名证书如果您要为自己的用途安装TLS服务器，则可能不希望前往CA获取公开信任的证书。 使用自签名证书要容易得多。 如果您是Firefox用户，则在首次访问该网站时，您可以创建证书例外，之后该站点将像使用公开信任的证书一样受到保护。 如果您已有CSR，请使用以下命令创建证书： 12345➜ openssl openssl x509 -req -days 356 -in fd.csr -signkey fd.key -out fd.crtSignature oksubject=/C=CN/ST=JS/L=SZ/O=USTC/OU=IT Department/CN=www.test.com/emailAddress=fangkui@ustc.edu.cnGetting Private keyEnter pass phrase for fd.key: 如果只是创建自签名证书，可以不用单独创建一个CSR，直接通过一条命令就可以生成证书： 12345678910111213141516➜ openssl openssl req -new -x509 -days 365 -key fd.key -out fd.crtEnter pass phrase for fd.key:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:JSLocality Name (eg, city) []:SZOrganization Name (eg, company) [Internet Widgits Pty Ltd]:USTCOrganizational Unit Name (eg, section) []:IT dpCommon Name (e.g. server FQDN or YOUR name) []:localhostEmail Address []:fangkui@ustc.edu.cn 使用如下命令查看证书内容： 1234567891011121314151617181920212223242526272829303132➜ openssl openssl x509 -text -in fd.crt -nooutCertificate: Data: Version: 3 (0x2) Serial Number: cf:71:94:b4:1e:57:ac:df Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=JS, L=SZ, O=USTC, OU=IT dp, CN=localhost/emailAddress=fangkui@ustc.edu.cn Validity Not Before: Dec 1 06:53:15 2018 GMT Not After : Dec 1 06:53:15 2019 GMT Subject: C=CN, ST=JS, L=SZ, O=USTC, OU=IT dp, CN=localhost/emailAddress=fangkui@ustc.edu.cn Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:eb:33:e5:5f:09:38:f9:12:3f:0f:dc:9d:69:16: [...] 0b:87 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 79:6F:A9:1A:88:E9:4C:73:B3:0A:E3:6F:70:B5:70:68:33:71:63:22 X509v3 Authority Key Identifier: keyid:79:6F:A9:1A:88:E9:4C:73:B3:0A:E3:6F:70:B5:70:68:33:71:63:22 X509v3 Basic Constraints: CA:TRUE Signature Algorithm: sha256WithRSAEncryption 61:68:e0:a1:36:d5:6a:97:35:b3:f9:7d:a2:5c:08:ab:20:14: [...] 39:00:7e:83 创建对多个域名有效的证书有两种方式可以实现一个证书对多个域名有效： 使用X.509提供的 Subject Alternative Name扩展罗列出所有的域名； 使用通配符（e.g., feistyduck.com and *.feistyduck.com）； 自签名证书通常只包含最基本的证书数据，如上例所示。相比之下，公共CA颁发的证书更有趣，因为它们包含许多其他字段（通过X.509扩展机制）。 Basic Constraints扩展用于将证书标记为属于CA，并给予它们签署其他证书的能力。非CA证书将省略此扩展名，或将CA的值设置为FALSE。 这种扩展很关键，这意味着所有软件必须理解其含义。 12X509v3 Basic Constraints: critical CA:FALSE Key Usage（KU）和 Extended Key Usage（EKU）扩展限制了证书的用途。如果存在这些扩展名，则仅允许列出的用途。如果扩展名不存在，则没有使用限制。您在此示例中看到的是Web服务器证书的典型内容，例如，它不允许进行代码签名： 1234X509v3 Key Usage: critical Digital Signature, Key EnciphermentX509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication CRL Distribution Points扩展列出了可以找到CA的证书吊销列表（CRL）信息的地址。 在需要撤销证书的情况下，此信息非常重要。 CRL是经过CA签名的撤销证书列表，以定期时间间隔（例如，七天）发布。 123X509v3 CRL Distribution Points:Full Name: URI:http://crl.starfieldtech.com/sfs3-20.crl Certificate Policies扩展用于指示颁发证书的策略。 例如，这是可以找到*extended validation *（EV）指标的地方（如以下示例）。 指示符采用唯一对象标识符（OID）的形式，并且它们对于颁发CA是唯一的。 此外，此扩展通常包含一个或多个证书策略声明（CPS）点，通常是网页或PDF文档。 123X509v3 Certificate Policies: Policy: 2.16.840.1.114414.1.7.23.3 CPS: http://certificates.starfieldtech.com/repository/ Authority Information Access（AIA）扩展通常包含两个重要信息。 首先，它列出了CA的在线证书状态协议（OCSP）响应程序的地址，该响应程序可用于实时检查证书撤销。 该扩展还可以包含指向发行人证书（链中的下一个证书）的位置的链接。 目前，服务器证书很少直接由受信任的根证书签名，这意味着用户必须在其配置中包含一个或多个中间证书。 错误很容易使证书无效。 某些客户端（例如，Internet Explorer）将使用此扩展中提供的信息来修复不完整的证书链，但许多客户端不会。 1234Authority Information Access: OCSP - URI:http://ocsp.starfieldtech.com/ CA Issuers - URI:http://certificates.starfieldtech.com/repository/sf… _intermediate.crt Subject Key Identifier和Authority Key Identifier扩展分别建立唯一的拥有者和授权密钥标识符。证书的Authority Key Identifier扩展中指定的值必须与颁发证书中Subject Key Identifier扩展中指定的值匹配。此信息在证书路径构建过程中非常有用，其中客户端尝试查找从叶（服务器）证书到受信任根的所有可能路径。证书颁发机构通常会使用一个带有多个证书的私钥，该字段允许软件可靠地识别哪个证书可以与哪个密钥匹配。在现实世界中，服务器提供的许多证书链都是无效的，但这一事实经常被忽视，因为浏览器能够找到替代的信任路径。 1234X509v3 Subject Key Identifier: 4A:AB:1C:C3:D3:4E:F7:5B:2B:59:71:AA:20:63:D6:C9:40:FB:14:F1X509v3 Authority Key Identifier: keyid:49:4B:52:27:D1:1B:BC:F2:A1:21:6A:62:7B:51:42:7A:8A:D7:D5:56 最后，使用Subject Alternative Name扩展名列出证书有效的所有主机名。此扩展程序曾经是可选的; 如果它不存在，客户端将回退到使用Common Name（CN）中提供的信息，该名称是“subject”字段的一部分。 如果存在扩展名，则在验证期间忽略CN字段的内容。 12X509v3 Subject Alternative Name: DNS:www.feistyduck.com, DNS:feistyduck.com","link":"/2018/12/01/openssl-cook-book/"},{"title":"模块打包Webpack","text":"Webpack学习笔记为什么使用Webpack现今的很多网页其实可以看做是功能丰富的应用，它们拥有着复杂的JavaScript代码和一大堆依赖包。为了简化开发的复杂度，前端社区涌现出了很多好的实践方法 模块化，让我们可以把复杂的程序细化为小的文件; 类似于TypeScript这种在JavaScript基础上拓展的开发语言：使我们能够实现目前版本的JavaScript不能直接使用的特性，并且之后还能转换为JavaScript文件使浏览器可以识别； Scss，less等CSS预处理器 … 这些改进确实大大的提高了我们的开发效率，但是利用它们开发的文件往往需要进行额外的处理才能让浏览器识别,而手动处理又是非常繁琐的，这就为WebPack类的工具的出现提供了需求。 什么是WebpackWebPack可以看做是模块打包机：它做的事情是，分析你的项目结构，找到JavaScript模块以及其它的一些浏览器不能直接运行的拓展语言（Scss，TypeScript等），并将其转换和打包为合适的格式供浏览器使用。","link":"/2018/05/29/%E6%A8%A1%E5%9D%97%E6%89%93%E5%8C%85Webpack/"},{"title":"硬盘分区","text":"硬盘分区硬盘结构的结构硬盘划分为磁头（Heads）、柱面(Cylinder)、扇区(Sector)。 △磁头(Heads)：每张磁片的正反两面各有一个磁头，一个磁头对应一张磁片的一个面。因此，用第几磁头就可以表示数据在哪个磁面。 △柱面(Cylinder)：所有磁片中半径相同的同心磁道构成“柱面”，意思是这一系列的磁道垂直叠在一起，就形成一个柱面的形状。简单地理解，柱面就是磁道。 △扇区(Sector)：将磁道划分为若干个小的区段，就是扇区。虽然很小，但实际是一个扇子的形状，故称为扇区。每个扇区的容量为512 bytes。 硬盘容量＝磁头数×柱面数×扇区数×512 bytes MBR分区方案硬盘的第一个扇区是整个硬盘最重要的部分，该扇区主要记录了主引导记录和分区表: 主引导记录(Master Boot Record, MBR)：可以安装启动管理程序的地方，占用446 bytes 分区表(Partition Table):记录整个硬盘分区状况，占用64 bytes 磁盘分区 硬盘默认的分区表最多只能划分四个主分区或者扩展分区（硬盘限制）。每个分区项占用16个字节，这16个字节中存有活动状态标志、文件系统标识、起止柱面号、磁头号、扇区号、隐含扇区数目(4个字节)、分区总扇区数目(4个字节)等内容。 分区的最小单位为柱面 扩展分区最多只能有一个(操作系统限制) 扩展分区能够划分出多个逻辑分区。逻辑分割的数量依操作系统而不同，在Linux系统中，IDE硬盘最多有59个逻辑分割(5号到63号)， SATA硬盘则有11个逻辑分割(5号到15号)。 存在的问题前面已经提到了主分区数目不能超过4个的限制，这是其一，很多时候，4个主分区并不能满足需要。另外最关键的是MBR分区方案无法支持超过2TB容量的磁盘。因为这一方案用4个字节存储分区的总扇区数，最大能表示2的32次方的扇区个数，按每扇区512字节计算，每个分区最大不能超过2TB。磁盘容量超过2TB以后，分区的起始位置也就无法表示了。 GUID分区方案GUID分区表(简称GPT。使用GUID分区表的磁盘称为GPT磁盘)是源自EFI标准的一种较新的磁盘分区表结构的标准。与目前普遍使用的主引导记录(MBR)分区方案相比，GPT提供了更加灵活的磁盘分区机制。它具有如下优点： 支持2TB以上的大硬盘。 每个磁盘的分区个数几乎没有限制。为什么说“几乎”呢？是因为Windows系统最多只允许划分128个分区。不过也完全够用了。 分区大小几乎没有限制。又是一个“几乎”。因为它用64位的整数表示扇区号。夸张一点说，一个64位整数能代表的分区大小已经是个“天文数字”了，若干年内你都无法见到这样大小的硬盘，更不用说分区了。 分区表自带备份。在磁盘的首尾部分分别保存了一份相同的分区表。其中一份被破坏后，可以通过另一份恢复。 每个分区可以有一个名称(不同于卷标)。","link":"/2018/04/28/%E7%A1%AC%E7%9B%98%E5%88%86%E5%8C%BA/"},{"title":"数据库事务","text":"Mysql事务模型Mysql中的InnoDB引擎支持事务,InnoDB支持四个标准定义的事务隔离级别: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE.默认的隔离级别是REPEATABLE READ. 在InnoDB中，所有用户活动都发生在事务中。如果启用了autocommit模式，则每个SQL语句自己形成一个事务。默认情况下，MySQL中的每一个连接的Session开启autocommit，因此如果该语句没有返回错误，MySQL会在每个SQL语句后执行提交。如果语句返回错误，则提交或回滚行为取决于错误类型。 启用了自动提交的会话可以通过使用显式START TRANSACTION或BEGIN语句启动它并以COMMIT或ROLLBACK语句结束它来执行多语句事务。 如果在SET autocommit = 0的会话中禁用了自动提交模式，则会话始终打开一个事务。 COMMIT或ROLLBACK语句结束当前事务并启动新事务. 如果Session中 autocommit 被禁用并且结束时未显式提交最终事务，则MySQL将回滚该事务。 某些语句隐式结束事务，就像您在执行语句之前完成了一个COMMIT一样。 JDBC事务如果JDBC连接处于自动提交模式，默认情况下，则每个SQL语句在完成后都会提交到数据库。要启用手动事务支持，而不是使用JDBC驱动程序默认使用的自动提交模式，请调用Connection对象的setAutoCommit()方法。 如果将布尔的false传递给setAutoCommit()，则关闭自动提交。 也可以传递一个布尔值true来重新打开它。 1conn.setAutoCommit(false); 完成更改后，若要提交更改，那么可在连接对象上调用commit()方法，如下所示： 1conn.commit( ); 否则，要使用连接名为conn的数据库回滚更新，请使用以下代码 - 1conn.rollback( );","link":"/2019/09/03/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/"},{"title":"科学代理","text":"科学代理服务端配置这里以 centos 7 操作系统为例，介绍如何搭建代理服务。 安装pip通过python的包管理器pip安装python版本的shadowsocks： 12curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\"python get-pip.py 安装配置shadowsocks在控制台执行以下命令安装 shadowsocks： 12pip install --upgrade pippip install shadowsocks 安装完成后，创建配置文件shadowsocks.json，内容如下： 123456{ \"server\": \"0.0.0.0\", \"server_port\": 8388, \"password\": \"uzon57jd0v869t7w\", \"method\": \"aes-256-cfb\"} 说明： method为加密方法，可选aes-128-cfb, aes-192-cfb, aes-256-cfb, bf-cfb, cast5-cfb, des-cfb, rc4-md5, chacha20, salsa20, rc4, table server_port为服务监听端口 password为密码，可使用密码生成工具生成一个随机密码 以上三项信息在配置 shadowsocks 客户端时需要配置一致，具体说明可查看 shadowsocks 的帮助文档。 配置自启动新建启动脚本文件/etc/systemd/system/shadowsocks.service，内容如下： 123456789[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c /etc/shadowsocks.json[Install]WantedBy=multi-user.target 执行以下命令启动 shadowsocks 服务： 12systemctl enable shadowsockssystemctl start shadowsocks 为了检查 shadowsocks 服务是否已成功启动，可以执行以下命令查看服务的状态： 1systemctl status shadowsocks -l 如果服务启动成功，则控制台显示的信息可能类似这样： 123456789101112● shadowsocks.service - Shadowsocks Loaded: loaded (/etc/systemd/system/shadowsocks.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2015-12-21 23:51:48 CST; 11min ago Main PID: 19334 (ssserver) CGroup: /system.slice/shadowsocks.service └─19334 /usr/bin/python /usr/bin/ssserver -c /etc/shadowsocks.jsonDec 21 23:51:48 morning.work systemd[1]: Started Shadowsocks.Dec 21 23:51:48 morning.work systemd[1]: Starting Shadowsocks...Dec 21 23:51:48 morning.work ssserver[19334]: INFO: loading config from /etc/shadowsocks.jsonDec 21 23:51:48 morning.work ssserver[19334]: 2015-12-21 23:51:48 INFO loading libcrypto from libcrypto.so.10Dec 21 23:51:48 morning.work ssserver[19334]: 2015-12-21 23:51:48 INFO starting server at 0.0.0.0:8388 客户端配置安装图形界面ubuntu 16.04版本可以直接通过软件源安装： 123sudo add-apt-repository ppa:hzwhuang/ss-qt5sudo apt update sudo apt install shadowsocks-qt5 其它Linux版本可以直接从 https://github.com/shadowsocks/shadowsocks-qt5/releases 下载最新版本的AppImage文件。在下载目录下执行： 12# form file dir./Shadowsocks-Qt5-3.0.1-x86_64.AppImage 文件名称替换为自己下载下来的文件名，如果没有执行权限，chmod a+x 修改权限。在图形界面配置好IP地址，密码，加密方式。需要和和服务器保持一致。 配置PAC模式使用genpac生成PAC文件： 123sudo pip install genpacgenpac --proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" -o autoproxy.pac --gfwlist-url=\"https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt\" 其中，gfwlist-url参数的地址在github仓库中 https://github.com/gfwlist/gfwlist.git 有详细介绍生成的pac文件保存在上面命令行 -o 参数指定的地方，如果没有指定路径，则默认保存在命令执行的当前路径下面。 然后在网络设置中添加PAC文件到动态代理模式：系统设置 &gt;&gt; 网络 &gt;&gt; 网络代理 &gt;&gt; 方法 &gt;&gt; 自动 在配置URL处填写file:// 后面跟你的pac文件路径，如图，然后点击应用到整个系统 chrome浏览器插件SwitchyOmega可以通过SwitchyOmega插件自定义代理解析，可以解决Linux下自定义gfw的问题。 脚本","link":"/2018/03/23/%E7%A7%91%E5%AD%A6%E4%BB%A3%E7%90%86/"},{"title":"记一次删除首个逻辑分区导致系统引导错误","text":"记一次误删分区问题事情的经过是这样，由于在安装Ubuntu/Windows7双系统时，Ubuntu分区规划不合理，导致在使用过程中/home目录空间严重不足。以下是我的分区方案： 123456789101112131415161718Disk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: dosDisk identifier: 0x00003702Device Boot Start End Sectors Size Id Type/dev/sda1 * 2048 206847 204800 100M 7 HPFS/NTFS/exFAT/dev/sda2 206848 209717247 209510400 99.9G 7 HPFS/NTFS/exFAT/dev/sda3 209717248 588597247 378880000 180.7G 7 HPFS/NTFS/exFAT/dev/sda4 588599294 976771071 388171778 185.1G f W95 Ext'd (LBA)/dev/sda5 588599296 767055871 178456576 85.1G 7 HPFS/NTFS/exFAT/dev/sda6 767057920 782680063 15622144 7.5G 82 Linux swap / Sol/dev/sda7 782682112 852680703 69998592 33.4G 83 Linux/dev/sda8 852682752 976771071 124088320 59.2G 83 LinuxPartition 4 does not start on physical sector boundary. 如上图所示，ubuntu的所属分区是通过创建一个扩展分区得到的。/dev/sda7为安装ubuntu挂载在/home目录。/dev/sda5是我从原来分配给windows的NTFS磁盘格式化过来，并现在被挂载在/home。 在操作的时候，直接通过图像界面删除了/dev/sda5分区，由于该分区为扩展分区第一个逻辑分区。扩展分区中逻辑驱动器的引导记录是链式的。每一个逻辑分区都有一个和主引导扇区结构类似的扩展引导记录(EBR)，其分区表的第一项指向该逻辑分区本身的引导扇区，第二项指向下一个逻辑驱动器的EBR，分区表第三、第四项没有用到。于是就导致我的整个扩展分区的逻辑分区都找不到了。ubuntu自然就不能被引导。 解决方案这里我的解决方案是，制作一个具有pe系统的U盘启动盘。保证pe中安装有DiskGenius分区工具。当然，在误操作分区的时候，一定不能对相应分区进行写操作。使用pe中的DiskGenius分区工具，可以直接回复误删分区。找回分区后，需要一个可用的Ubuntu Live。可以刻录镜像到U盘，也可以直接使用Ubuntu 光盘。进入Ubuntu Live。挂载所有Ubuntu的分区，然后chroot到挂载后的分区中，重新安装grub。重启就可以了。 1234567891011121314151617# 切换root用户sudo -i# 我的ubuntu中 /dev/sda8挂载在 / 目录的mount /dev/sda8 /mnt# 我的ubuntu中 /dev/sda8挂载在 /home 目录mount /dev/sda7 /mnt/home# 挂载系统设备mount –bind /proc /mnt/procmount –bind /dev /mnt/devmount –bind /sys /mnt/sys# 切换root目录chroot /mnt# 重新安装grubgrub-install /dev/sdaupdate-grub","link":"/2018/08/09/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%88%A0%E9%99%A4%E9%A6%96%E4%B8%AA%E9%80%BB%E8%BE%91%E5%88%86%E5%8C%BA%E5%AF%BC%E8%87%B4%E7%B3%BB%E7%BB%9F%E5%BC%95%E5%AF%BC%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"学习笔记","slug":"学习笔记","link":"/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Next","slug":"Next","link":"/tags/Next/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Mybatis","slug":"Mybatis","link":"/tags/Mybatis/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"OpenWAF","slug":"OpenWAF","link":"/tags/OpenWAF/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"Tomcat 安装","slug":"Tomcat-安装","link":"/tags/Tomcat-%E5%AE%89%E8%A3%85/"},{"name":"WAF","slug":"WAF","link":"/tags/WAF/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"WebSocket","slug":"WebSocket","link":"/tags/WebSocket/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/tags/Spring-Boot/"},{"name":"maven","slug":"maven","link":"/tags/maven/"},{"name":"-HTTPS -openSSL","slug":"HTTPS-openSSL","link":"/tags/HTTPS-openSSL/"},{"name":"前端框架及工具","slug":"前端框架及工具","link":"/tags/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E5%8F%8A%E5%B7%A5%E5%85%B7/"},{"name":"事务","slug":"事务","link":"/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"JDBC","slug":"JDBC","link":"/tags/JDBC/"},{"name":"代理","slug":"代理","link":"/tags/%E4%BB%A3%E7%90%86/"},{"name":"OS","slug":"OS","link":"/tags/OS/"}],"categories":[]}